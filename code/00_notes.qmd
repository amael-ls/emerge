---
title: "Exploring the challenges"
date: today
author: Amaël Le Squin
date-format: iso
filters:
  - fontawesome
execute:
  error: true
bibliography: references.bib
format:
  html:
    toc: true
    include-in-header: mathjax.html
    code-fold: true
    df-print: paged
    theme:
      light: cerulean
      dark: darkly
    margin: 5% 0;
  pdf:
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{siunitx}
---

\newcommand{\ie}{*i.e.,*}
\newcommand{\F}{\mathscr{F}}
\newcommand{\N}{\mathbfscr{N}}
\renewcommand{\epsilon}{\varepsilon}

::: {.callout-important}
## Available on Gitlab
This note is available on [{{< fa brands gitlab title="Amaël's gitlab at IGN" >}}gitlab](https://gitlab.ign.fr/ale-squin/tarifs-emerge).
:::

## Introduction
In order to test my models, I first run dummy tests, with generated data. I build models slowly, from the simplest to the most complex. The first challenge is related to the model used by Christine Deleuze *et al*. (scripts given on 11 October 2024):
```{r}
#| eval: false
Modele_mai2 = nlme(formTotNew ~ a + b*hdn + d*hsurd, data = Grdata.PV,
	start = c(a = 0.4, 0, b = 1.5, 0, d = 0.0005, 0),
	fixed = list(a + b + d ~ feuil.res), random = a + d ~ 1|nomessence2)
```
Few things raised a flag in my mind:

1. The R package `nlme`, non-linear mixed effect, is for repeated measured data and is based on @Lindstrom1990. Here, the repetition occurs within the species level, so I am not sure this package is the best adapted... I feel that `lme4` would have been better
2. `hdn` is the hardiness $\sqrt{c}/h$ while `hsurd` is what I call the 'slenderness', $h/c$. Therefore, there is a high (negative) correlation between these two explanatory variables!

The model `Modele_mai2` should be understood as follows, for an individual of height $h$ and circumference at breast height $c$, of species $j$ and functional group (conifer or broadleaf) $i$:
\begin{equation} \label{eq::deleuze}
\begin{aligned}
	\F &\sim \N(\mu, \sigma) \\
	\mu_{i, j} &= a_{i, j} + b_i \frac{\sqrt{c}}{h} + d_{i, j} \frac{h}{c} \\
	a_{i, j} &\sim \N(\alpha_i, \sigma_{\alpha}) \\
	d_{i, j} &\sim \N(\delta_i, \sigma_{\delta}),
\end{aligned}
\end{equation}
where $\alpha_i$ and $\delta_i$ are the 'group intercepts' (common values within conifers and broadleaves). Therefore, it seems to be a GLMM, and I am not sure `nlme` is relevant here. In the next sections, I will try to reproduce their model with generated data. I will do it step-by-step.

## Generating data
### Packages and helpers
First, I load the necessary packages:
```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

options(max.print = 500)

library(data.table)
library(cmdstanr)
library(stringi)
library(gt)
```

Then, I define useful functions:
```{r}
#### Tool functions
## Tools
source("./toolFunctions.R")

## Generate k integers with constraint they sum to n >= k
generate_random_partition = function(n, k)
{
	if (n < k)
		stop("n should be larger than k")
	parts = c(0, sort(sample(1:(n - 1), k - 1)), n)
	return(diff(parts))
}
```

### Parameters
I define the following parameters, that will be used to generate data according to equation \eqref{eq::deleuze}:
```{r}

#### Define parameters
set.seed(1969 - 08 - 18) # Woodstock seed

## Fixed effects
b0 = c(conif = 0.2, broad = 5.3)
b1 = c(conif = 0.27, broad = 1.4)
b2 = c(conif = 4, broad = 0.21)

## Variance for random effects and residuals
sigma_beta0 = 4.2
sigma_beta2 = 6.98
sigma = 1.2

## Number of data and species
n = 3e3
S = 30 # Number of species
```
We aim to recover them later with a statiscal model.

### Data
```{r}
rep_species = generate_random_partition(n, S)

lim_broadleaf = sum(rep_species[1:15]) + 1

n_conif = lim_broadleaf - 1
n_broad = n - n_conif

fake_dt = data.table(
	species = rep(paste0("sp_", 1:S), times = rep_species),
	type = c(rep("conif", n_conif), rep("broad", n_broad)),
	fake_hdn = runif(n = n, min = -10, max = 50),
	fake_slenderness = rnorm(n = n, mean = 0, sd = 20),
	b0 = c(rep(b0["conif"], n_conif), rep(b0["broad"], n_broad)),
	b1 = c(rep(b1["conif"], n_conif), rep(b1["broad"], n_broad)),
	b2 = c(rep(b2["conif"], n_conif), rep(b2["broad"], n_broad)))

fake_dt[, beta0 := rnorm(1, unique(b0), sigma_beta0), by = species]
fake_dt[, beta2 := rnorm(1, unique(b2), sigma_beta2), by = species]

# fake_dt[, table(round(beta0, 2))]
# fake_dt[, table(round(beta2, 2))]

# fake_dt[, table(b0)]
# fake_dt[, table(b1)]
# fake_dt[, table(b2)]

fake_dt[, fake_mu := beta0 + b1*fake_hdn + beta2*fake_slenderness]
fake_dt[, cor(fake_hdn, fake_slenderness)]
# fake_dt[, fake_vol := rnorm(.N, fake_mu, sigma)]

for (i in 1:n)
	fake_dt[i, fake_vol := rnorm(1, fake_mu, sigma)]

fake_dt[, sd(fake_mu), by = species]

ind_species = fake_dt[, .(start = .I[1], end = .I[.N]), by = .(species)]
ind_species[, n_indiv := end - start + 1, by = species]
ind_species[, sum(n_indiv)] == n
```