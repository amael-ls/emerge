---
title: "Exploring the challenges"
date: today
author: Amaël Le Squin
date-format: iso
filters:
  - fontawesome
execute:
  error: true
bibliography: references.bib
css: style.css
format:
  html:
    toc: true
    include-in-header: mathjax.html
    code-fold: true
    df-print: paged
    theme:
      light: cerulean
      dark: darkly
    margin: 5% 0;
  pdf:
    keep-tex: true
    pdf-engine: lualatex
    include-in-header:
      - text: |
          \usepackage{unicode-math}
---

\newcommand{\ie}{*i.e.,*}
\newcommand{\F}{\mathscr{F}}
\newcommand{\N}{\mathbfscr{N}}
\renewcommand{\epsilon}{\varepsilon}

::: {.callout-important}
## Available on Gitlab
This note is available on [{{< fa brands gitlab title="Amaël's gitlab at IGN" >}}gitlab](https://gitlab.ign.fr/ale-squin/tarifs-emerge).
:::

## Introduction
In order to test my models, I first run dummy tests, with generated data. I build models slowly, from the simplest to the most complex. The first challenge is related to the model used by Christine Deleuze *et al*. (scripts given on 11 October 2024):
```{r}
#| eval: false
Modele_mai2 = nlme(formTotNew ~ a + b*hdn + d*hsurd, data = Grdata.PV,
	start = c(a = 0.4, 0, b = 1.5, 0, d = 0.0005, 0),
	fixed = list(a + b + d ~ feuil.res), random = a + d ~ 1|nomessence2)
```
Few things raised a flag in my mind:

1. The R package `nlme`, non-linear mixed effect, is for repeated measured data and is based on @Lindstrom1990. Here, the repetition occurs within the species level, so I am not sure this package is the best adapted... I feel that `lme4` would have been better
2. `hdn` is the hardiness $\sqrt{c}/h$ while `hsurd` is what I call the 'slenderness', $h/c$. Therefore, there is a high (negative) correlation between these two explanatory variables!

The model `Modele_mai2` should be understood as follows, for an individual of height $h$ and circumference at breast height $c$, of species $j$ and functional group (conifer or broadleaf) $i$:
\begin{equation} \label{eq::deleuze}
\begin{aligned}
	\F &\sim \N(\mu, \sigma) \\
	\mu_{i, j} &= a_{i, j} + b_i \frac{\sqrt{c}}{h} + d_{i, j} \frac{h}{c} \\
	a_{i, j} &\sim \N(\alpha_i, \sigma_{\alpha}) \\
	d_{i, j} &\sim \N(\delta_i, \sigma_{\delta}),
\end{aligned}
\end{equation}
where $\alpha_i$ and $\delta_i$ are the 'group intercepts' (common values within conifers and broadleaves). Therefore, it seems to be a GLMM, and I am not sure `nlme` is relevant here. In the next sections, I will try to reproduce their model with generated data. I will do it step-by-step.

## Generating data
### Packages and helpers
First, I load the necessary packages:
```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

options(max.print = 500)

library(data.table)
library(cmdstanr)
	register_knitr_engine(override = TRUE)
library(stringi)
library(gt)
```

::: {.callout-tip}
## Available on Gitlab
By default, Quarto uses the knitr's built-in stan engine `rstan`. To override it so that all stan chunks are processed with `CmdStanR`, I need to specify:
```{r}
#| eval: false
#| code-fold: false
register_knitr_engine(override = TRUE)
```
:::

Then, I define useful functions:
```{r}
#### Tool functions
## Tools
source("./toolFunctions.R")

## Generate k integers with constraint they sum to n >= k
generate_random_partition = function(n, k)
{
	if (n < k)
		stop("n should be larger than k")
	parts = c(0, sort(sample(1:(n - 1), k - 1)), n)
	return(diff(parts))
}
```

### Parameters
I define the following parameters, that will be used to generate data according to equation \eqref{eq::deleuze}:
```{r}
#### Define parameters
set.seed(1969 - 08 - 18) # Woodstock seed

## Fixed effects
b0 = c(conif = 0.2, broad = 5.3)
b1 = c(conif = 0.27, broad = 1.4)
b2 = c(conif = 4, broad = 0.21)

## Variance for random effects and residuals
sigma_beta0 = 4.2
sigma_beta2 = 6.98
sigma = 1.2

## Number of data and species
n = 3e3
S = 30 # Number of species
```
We aim to recover them later with a statiscal model.

### Data
```{r}
#| output: false
#| hold: true
rep_species = generate_random_partition(n, S)

lim_broadleaf = sum(rep_species[1:19]) + 1

n_conif = lim_broadleaf - 1
n_broad = n - n_conif

fake_dt = data.table(
	species = rep(paste0("sp_", 1:S), times = rep_species),
	type = c(rep("conif", n_conif), rep("broad", n_broad)),
	fake_hdn = runif(n = n, min = -10, max = 50),
	fake_slenderness = rnorm(n = n, mean = 0, sd = 20),
	b0 = c(rep(b0["conif"], n_conif), rep(b0["broad"], n_broad)),
	b1 = c(rep(b1["conif"], n_conif), rep(b1["broad"], n_broad)),
	b2 = c(rep(b2["conif"], n_conif), rep(b2["broad"], n_broad)))

fake_dt[, beta0 := rnorm(1, unique(b0), sigma_beta0), by = species]
fake_dt[, beta2 := rnorm(1, unique(b2), sigma_beta2), by = species]

b0m = fake_dt[, round(mean(unique(beta0)), 3), by = type]
b0m[, sig := fake_dt[, round(sd(unique(beta0)), 3)]]
setkey(b0m, type)

b2m = fake_dt[, round(mean(unique(beta2)), 3), by = type]
b2m[, sig := fake_dt[, round(sd(unique(beta2)), 3)]]
setkey(b2m, type)

fake_dt[, fake_mu := beta0 + b1*fake_hdn + beta2*fake_slenderness]
fake_dt[, fake_vol := rnorm(.N, fake_mu, sigma)]
fake_dt[, fake_vol_no_randeff := rnorm(.N, b0 + b1*fake_hdn + b2*fake_slenderness, sigma)]

ind_species = fake_dt[, .(start = .I[1], end = .I[.N]), by = .(species)]
ind_species[, n_indiv := end - start + 1, by = species]
ind_species[, sum(n_indiv)] == n

temporary = lm(fake_dt[, fake_vol] ~ 0 + fake_dt[, fake_mu])
sig_est = round(summary(temporary)$sigma, 3)
```

Here is a summary of the parameters value $\beta_0$ and $\beta_2$ (simulated), and $b_0$ and $b_2$, which are supposed to be the mean of $\beta_0$s and $\beta_2$s:

| **Parameter**            | **Conifer**             | **Broadleaf**           |
|-------------------------:|:------------------------|:------------------------|
| $b_0$                    | `{r} b0["conif"]`       | `{r} b0["broad"]`       |
| $\beta_0$                | `{r} b0m["conif", V1]`  | `{r} b0m["broad", V1]`  |
| $\sigma_{\alpha}$        | `{r} sigma_beta0`       | `{r} sigma_beta0`       |
| $\sigma_{\alpha}$ (data) | `{r} b0m["conif", sig]` | `{r} b0m["broad", sig]` |
| $b_1$                    | `{r} b1["conif"]`       | `{r} b1["broad"]`       |
| $b_2$                    | `{r} b2["conif"]`       | `{r} b2["broad"]`       |
| $\beta_2$                | `{r} b2m["conif", V1]`  | `{r} b2m["broad", V1]`  |
| $\sigma_{\delta}$        | `{r} sigma_beta2`       | `{r} sigma_beta2`       |
| $\sigma_{\delta}$ (data) | `{r} b2m["conif", sig]` | `{r} b2m["broad", sig]` |
| $\sigma$                 | `{r} sigma`             | `{r} sigma`             |
| $\sigma$ (lm)            | `{r} sig_est`           | `{r} sig_est`           |
: Parameters value for both functional groups {.borderless .hover}

As can be seen, the data do not necessarily represent the true parameters value very well.

## Parameter recovery

::: {.callout-tip}
## Caching execution
To store some results that could be slow to obtain, or to avoid the recompilation of stan code, use the option `cache = TRUE`.
:::

### Simplest model
I first define the simplest possible model, where there is no hierarchy, and I only try to estimate $b_0$, $b_1$, $b_2$, and the residual variance $\sigma$:

::: {#lst-simple_mod}

```{stan output.var = "simplest_model", cache = TRUE}
data {
	// Dimensions and indices
	int N; // Number of individuals
	int lim_broadleaf; // Number of individuals

	// Predictors
	vector [N] fake_hdn;
	vector [N] fake_slenderness;

	// Response variable
	vector[N] volume_m3;
}

parameters {
	// Fixed effects
	vector[2] b0;
	vector[2] b1;
	vector[2] b2;

	// Variance
	real<lower = 0> sigma; // sd residuals
}

model {
	// Priors
	target += normal_lpdf(b0 | 0, 10);
	target += normal_lpdf(b1 | 0, 10);
	target += normal_lpdf(b2 | 0, 10);

	target += inv_gamma_lpdf(sigma | 1, 1);

	target += normal_lpdf(volume_m3[1:(lim_broadleaf - 1)] | b0[1] +
		b1[1]*fake_hdn[1:(lim_broadleaf - 1)] +
		b2[1]*fake_slenderness[1:(lim_broadleaf - 1)], sigma);

	target += normal_lpdf(volume_m3[lim_broadleaf:N] | b0[2] +
		b1[2]*fake_hdn[lim_broadleaf:N] +
		b2[2]*fake_slenderness[lim_broadleaf:N], sigma);
}
```

:::

Prepare the data for Stan,
```{r}
#### Stan data
## Data list
stanData = list(
	N = fake_dt[, .N],
	S = S,
	ind_start_sp = ind_species[, start],
	ind_end_sp = ind_species[, end],
	lim_broadleaf = lim_broadleaf,
	fake_hdn = fake_dt[, fake_hdn],
	fake_slenderness = fake_dt[, fake_slenderness],
	volume_m3 = fake_dt[, fake_vol_no_randeff]
)

## Common variables
n_chains = 4
iter_warmup = 500
iter_sampling = 1500
```
and run the simplest model (see @lst-simple_mod):
```{r, cache = TRUE}
#| output: false
#| echo: false
fit = simplest_model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	refresh = 200, iter_warmup = iter_warmup, iter_sampling = iter_sampling)
fit$save_output_files(dir = "./", basename = paste0("simplest_model"), random = FALSE)
```

```{r}
#| html-table-processing: none
info_dt = pretty_summary(fit = fit, params = fit$metadata()$model_params[-1]) # -1 to remove lp__
info_dt |>
	gt() |>
	cols_label(
		params_name = "Parameter",
		mean_params = "Mean",
		sd_params = "Std. dev",
		r_hat_params = "r hat"
	) |>
	fmt_number(
		n_sigfig = 2
	) |>
	tab_style(
		style = cell_borders(sides = "all", style = NULL),
		locations = cells_body()
	) |>
	tab_style(
		style = cell_text(weight = "bold"),
		locations = list(cells_column_labels(), cells_column_spanners())
	) |>
	tab_style(
		style = cell_text(align = "right"),
		locations = cells_body(columns = params_name)
	)
```

