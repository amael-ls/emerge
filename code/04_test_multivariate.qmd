---
title: "Simple examples of multivariate models"
date: today
author: AmaÃ«l Le Squin
date-format: iso
filters:
  - fontawesome
  - siunitx-quarto
  - imagify
imagify:
  header-includes: |
    \usepackage{pgfplots}
    \pgfplotsset{compat=1.18}
    \usepgfplotslibrary{fillbetween}
    \usetikzlibrary{positioning}
    \usetikzlibrary{calc}
  pdf-engine: lualatex
pdf-engine: lualatex
keep-tex: true
execute:
  error: true
bibliography: /home/ALe-Squin/work/library/bib_file/references.bib
lightbox:
  match: auto
css: style.css
knitr:
  opts_chunk: 
    dev: ragg_png
    crop: null
    out.width: "70%"
    fig.width: 6
    fig.asp: 0.618
    fig.align: "center"
format:
  html:
    toc: true
    include-in-header: mathjax.html
    code-fold: true
    df-print: paged
    number-sections: true
    theme:
      light: cerulean
      dark: darkly
    margin: 5% 0;
  pdf:
    mathspec: true
    include-in-header:
      - text: |
          \usepackage{unicode-math}
---

<!-- Shortcut for text -->
\newcommand{\ie}{*i.e.,*}

<!-- Shortcut for math -->
\newcommand{\bOmega}{\symbf{\Omega}}
\newcommand{\bSigma}{\symbf{\Sigma}}

\newcommand{\alphai}{\alpha_{\text{actor}[i], \text{TID}[i]}}
\newcommand{\betai}{\beta_{\text{block}[i], \text{TID}[i]}}
\newcommand{\gammai}{\gamma_{\text{TID}[i]}}

<!-- New symbols -->
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

<!-- Math operators -->
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\mvn}{MVN}
\DeclareMathOperator{\N}{\mathcal{N}}
\DeclareMathOperator{\poiss}{Poisson}

## Introduction

The aim of this small file, which might later become an appendix of the main article, is to explore two simple multivariate examples in Stan language. I base my code on two examples developed by @McElreath2020 (chap. 14) about (*i*) a cafe robot and (*ii*) chimpanzees.

### Cafes robot example
Let a robot visit cafes and order a coffee. The robot measures each time how long did it wait, and when it arrived (a simple categorical variable of two values: morning and afternoon). In the chapter 13 of @McElreath2020, it has been shown that the robot learns better when using partial pooling. In the chapter 14, McElreath claims that we can learn even more by also investigating and accounting for the correlation structure. Why? Because popular cafes have a longer waiting time in the morning (rush hour) and improves significantly the afternoon, while there is little difference in waiting time between the morning and the afternoon for non-popular cafes. Therefore, there are two levels of borrowing information:

1. Between the cafes (partial pooling)
2. Between the parameters (covariance)

The @sec-create_data is dedicated to simulate data, which are fit in @sec-fit by jointly modelling slopes and intercepts. The results are analysed in @sec-results.

### Chimpanzees example
The chimpanzee experiment is fully described in @McElreath2020 (chap. XI). Basically, there is a table with two dishes and a lever on the left and right side, respectively. Pulling a lever moves a dish towards you, and move the other dish towards the other side of the table. On one side, only one dish contains food, which comes to you if you pull the corresponding lever, and on the other side both dishes contains food. The aim is to see if a chimpanzees behave differently when there is a congener at the other end of table or not. Humans, for instance, would pull the lever that provides food to both, which is called the *prosocial* option.

The data are described in @sec-data_chimp and are fit in @sec-fit_chimp. The results are analysed in @sec-results_chimp.

### Spatial autocorrelation in oceanic tools


## Cafes robot
### Create data {#sec-create_data}

```{r}
#| output: false
#### Clear space and load packages
# rm(list = ls())
graphics.off()

options(max.print = 500)

library(data.table)
library(rethinking)
# library(WorldFlora)
library(MetBrewer)
library(cmdstanr)
	register_knitr_engine(override = TRUE)
library(stringi)
library(ellipse)
library(DHARMa)
# library(taxize)
library(terra)
library(MASS)
library(gt)

source("./toolFunctions.R")
```

The parameters are defined as below:
```{r}
#### Population parameters, p. 437
a = 3.5 # Mean intercept (i.e., mean waiting time)
b = -1 # Mean slope (i.e., mean improvement waiting time in the afternoon. Improvement because < 0)

sigma_a = 1 # Variance intercept
sigma_b = 0.5 # Variance change morning/afternoon (improvement if < 0, worsen otherwise)
sigma_cafe = 0.15 # Variance within cafe

rho = -0.7 # Covariance between intercepts and slopes
# (i.e., the more you wait in the morning, the more it improves in the afternoon)

#### Var-Cov matrix
sigma_mat = diag(c(sigma_a, sigma_b))
rho_mat = matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)

varCov_mat = sigma_mat %*% rho_mat %*% sigma_mat
```

and the data are created using a multivariate normal distribution for the parameters (intercept and slope for each cafe, all coming from the same distribution---group effect and correlation are then included):

```{r}
#### Simulate data
## Simulate parameters
n_cafes = 40
set.seed(5)

params_dt = as.data.table(mvrnorm(n_cafes, c(a, b), varCov_mat))
setnames(params_dt, new = c("alpha_cafe", "beta_cafe"))

plot(params_dt[, alpha_cafe], params_dt[, beta_cafe], pch = 19, xlab = "alpha", ylab = "beta", axes = FALSE)
axis(1)
axis(2, las = 1)
for (l in c(0.1, 0.3, 0.5, 0.8, 0.99))
	lines(ellipse(x = varCov_mat, centre = c(a, b), level = l), col = "#22442233", lwd = 2)

## Simulate waiting times
set.seed(22)
n_visit = 10 # 5 in the morning, 5 in the afternoon

if (n_visit %% 2 != 0)
	stop("n_visit must be an even integer")

visit_dt = data.table(cafe = rep(1:n_cafes, each = n_visit),
	is_afternoon = rep(c(rep(FALSE, n_visit/2), rep(TRUE, n_visit/2)), n_cafes))
setkey(visit_dt, cafe)

for (i in 1:n_cafes)
	visit_dt[.(i), waiting_time := rnorm(n = n_visit,
		mean = params_dt[i, alpha_cafe] + params_dt[i, beta_cafe]*is_afternoon, sd = sigma_cafe)]

visit_dt[, afternoon_int := ifelse(is_afternoon, 1, 0)]
```

This creates an easy, balanced dataset of `{r} n_cafes` cafes visited `{r} n_visit` times (equaly shared between mornings and afternoons).

### Fit the model {#sec-fit}
#### DAG
```{mermaid}
%%| label: fig-model
%%| fig-cap: Diagram Acyclic Graph
graph TB
	Y("$$Y \sim N \left( \mu_{\text{cafe}[i]}, \sigma_{\text{cafe}[i]} \right)$$")
```

#### Stan model
Here is the stan model according to the @fig-model:
```{stan output.var = "model", cache = TRUE, fold = FALSE}
data {
	// Dimensions
	int <lower = 1> N_cafes; // Number of cafes
	int <lower = 1> N_visit; // Number of visits per cafe
	int <lower = N_cafes*N_visit, upper = N_cafes*N_visit> N; // Number of data

	// Explanatory variable
	vector <lower = 0, upper = 1> [N] afternoon; // 0 if morning, 1 otherwise

	// Observations
	vector <lower = 0> [N] wait;
}

parameters {
	// Population parameters...
	// ... intercept and slope
	real a; // Intercept
	real b; // Slope
	
	// ... variances
	real sigma_a;
	real sigma_b;
	cholesky_factor_corr[2] L;

	// Variance (residuals)
	real<lower = 0> sigma; // Variance within same cafe

	// Whatever
	vector [N_cafes] a_cafe;
	vector [N_cafes] b_cafe;
}

transformed parameters {
	vector <lower = 0>[2] sigma_vec = to_vector({sigma_a, sigma_b});
}

model {
	int cc = 0;

	// Population priors
	target += normal_lpdf(a | 5, 2);
	target += normal_lpdf(b | -1, 2);
	
	target +=  lkj_corr_cholesky_lpdf(L | 2); // It contains rho (non-diag)

	// Variance (residuals)
	target += exponential_lpdf(sigma | 1);

	// Likelihood and cafe parameters
	for (i in 1:N_cafes)
	{
		target += multi_normal_cholesky_lpdf(to_vector({a_cafe[i], b_cafe[i]}) | to_vector({a, b}),
			diag_pre_multiply(sigma_vec, L)); // Cafe params
		for (j in 1:N_visit)
		{
			cc = (i - 1)*N_visit + j;
			target += normal_lpdf(wait[cc] | a_cafe[i] + b_cafe[i]*afternoon[cc], sigma); // Likelihood
		}
	}
}

generated quantities {
	cov_matrix[2] Sigma;
	real rho;
	
	// This is to recover the variance-covariance matrix instead of having its Cholesky factor
	Sigma = multiply_lower_tri_self_transpose(diag_pre_multiply(sigma_vec, L));

	// This is to recover the correlation parameter
	rho = tcrossprod(L)[2, 1];
}
```

All the functions are described in the [Stan Functions Reference](https://mc-stan.org/docs/functions-reference/), however, few explanations are required for the multivariate.

Remember that the variance-covariance matrix of a multivariate distribution, $\bSigma$, can be rewritten as a product of 'pure variance' and 'pure covariance' matrices:
$$
\bSigma = \begin{bmatrix} \sigma_a & 0 \\ 0 & \sigma_b \end{bmatrix}
	\begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}
	\begin{bmatrix} \sigma_a & 0 \\ 0 & \sigma_b \end{bmatrix}
$$

The diagonal elements of $\bSigma$ are the variances of the parameters ($\sigma_a^2$ and $\sigma_b^2$), and the non-diagonal elements are the covariances ($\rho \sigma_a \sigma_b$). We denote the diagonal matrix by $\symbf{V}$, and the middle matrix by $\bOmega = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}$. The matrix $\bOmega$ is symmetric positive-definite and can be Cholesky-decomposed:
$$
\bOmega = \symbf{L}\symbf{L}^T,
$$
where the index $T$ is for transpose, and $L$ is a lower triangular matrix. According to [Stan correlation matrix page](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html), it is much better computationally to work directly with the Cholesky factor of $\bSigma$, which is:
\begin{equation}
	\label{eq::cholesky_Sigma}
	\begin{aligned}
		\bSigma &= \symbf{V} \bOmega \symbf{V} \\
			&= \symbf{V} \symbf{L} \symbf{L}^T \symbf{V} \\
			&= (\symbf{V} \symbf{L}) (\symbf{V} \symbf{L})^T.
	\end{aligned}
\end{equation}
We denote by $\symbf{L_{\Sigma}} = \symbf{V} \symbf{L}$ the Cholesky factor of $\bSigma$.

Now that this is explained, the line `target +=  lkj_corr_cholesky_lpdf(L | 2);` is just the prior on $\symbf{L}$. The prior `target +=  lkj_corr_lpdf(\eta)` just put a prior that is skeptical about high correlation for $\eta \geqslant 1$. The higher $\eta$, the more concentrated around the identity matrix is the prior [@McElreath2020]. [Stan correlation matrix page](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html) recommands to work with the Cholesky decomposition, hence the prior on $\symbf{L}$ rather than $\bOmega$ and the use of `lkj_corr_cholesky_lpdf` instead of `lkj_corr_lpdf` (note the _cholesky_ that disappeared in the name).

Then the second step is `diag_pre_multiply(sigma_vec, L)`. This function just transforms a vector (here $(\sigma_a, \sigma_b)$) to a diagonal matrix and then multiply it by $\symbf{L}$, which gives $\symbf{L_{\Sigma}}$ (see equation \eqref{eq::cholesky_Sigma}). Note that this is valid only because I use `multi_normal_cholesky_lpdf` which requires the Cholesky factor of $\bSigma$, $\symbf{L_{\Sigma}}$, contrary to `multi_normal_lpdf`.

#### Stan data
```{r}
#### Fit the model
## Stan data
stanData = list(
	N_cafes = n_cafes, # Number of cafes
	N_visit = n_visit, # Number of visits per cafe
	N = n_cafes*n_visit,
	afternoon = visit_dt[, afternoon_int],
	wait = visit_dt[, waiting_time]
)

## Common variables
n_chains = 4

iter_warmup = 1000
iter_sampling = 1000
```

#### Run the model
```{r}
#| output: false
#| fold: false
## Run
fit = model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)
```

### Results {#sec-results}
The posterior of $\rho$ (which equals to `{r} rho`) can be extracted by multiplying $\symbf{L}$ by its transpose, and then take the subdiagonal value $\symbf{L}\symbf{L}^T[2, 1]$:
```{r}
rho_posterior = fit$draws("rho", inc_warmup = FALSE)
aa = lazyPosterior(rho_posterior, val1 = rho)
```

The posterior of the population parameters are:
```{r}
#| echo: false
#| label: fig-posteriorPop
#| fig-cap: "Posterior of the population parameters"
#| fig-subcap: 
#|   - "$a$"
#|   - "$b$"
#| layout-ncol: 2
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("a", inc_warmup = FALSE), val1 = a)
aa = lazyPosterior(fit$draws("b", inc_warmup = FALSE), val1 = b)
```

The posterior of the variances are:
```{r}
#| echo: false
#| label: fig-posteriorVar
#| fig-cap: "Posterior of the variances"
#| fig-subcap: 
#|   - "$\\sigma_a$"
#|   - "$\\sigma_b$"
#|   - "$\\sigma$"
#| layout-ncol: 3
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("sigma_a", inc_warmup = FALSE), val1 = sigma_a)
aa = lazyPosterior(fit$draws("sigma_b", inc_warmup = FALSE), val1 = sigma_b)
aa = lazyPosterior(fit$draws("sigma", inc_warmup = FALSE), val1 = sigma_cafe)
```

And the posterior of the covariance (which is $\rho \sigma_a \sigma_b$)
```{r}
#| echo: false
#| label: fig-posteriorCov
#| fig-cap: "Posterior of the covariance"
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("Sigma")[, , "Sigma[2,1]"], val1 = sigma_a*sigma_b*rho)
```

## Chimpanzee
### Data description {#sec-data_chimp}
```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

source("./toolFunctions.R")

data(chimpanzees)
setDT(chimpanzees)

n_chimp = length(unique(chimpanzees[, actor]))
n_block = length(unique(chimpanzees[, block]))
```

There are eight variables in the `chimpanzees` dataset:

1. actor: name of the target chimpanzee (which pull the levers)
2. recipient: name of recipient (NA for partner absent condition)
3. condition: partner absent (0), partner present (1)
4. block: block of trials (each actor x each recipient 1 time)
5. trial: trial number (by chimp = ordinal sequence of trials for each chimp, ranges from 1-72; partner present trials were interspersed with partner absent trials)
6. prosocial_left: 1 if prosocial (1/1) option was on left
7. chose_prosoc: choice chimp made (0 = 1/0 option, 1 = 1/1 option)
8. pulled_left: which side did chimp pull (1 = left, 0 = right)

This dataset is **cross-classified** because actors are not nested within blocks. The `block` represents the batch of experiment, I think this variable could have been called `day` or `batch_id`. Within a day, all the chimpanzees will be actor $2(n - 1)$ times: $n - 1$ times alone, and $n - 1$ times with a partner, where $n$ is the number of chimpanzees. For the recipient, I think they do a $+1$ to the id of the chimpanzees: a chimpanzee $i$ will be in relation with all the other chimpanzee but $i + 1$ (which is I guess himself...). That's a bit odd...

In total, there are `{r} n_chimp` chimpanzees and `{r} n_block` blocks (or batches of experiment).

### Fit the model {#sec-fit_chimp}
We are interested in predicting `pulled_left`, $L_i$, which indicates if the studied chimpanzee pulled the left lever (value 1) or the right lever (value 0). The predictors are `prosoc_left`, which indicates if the *prosocial* choice was associated to the left lever, and `condition`, which indicates the congener presence. That creates four treatments:

1. R/N: Right lever is the prosocial choice, no partner
2. L/N: Left lever is the prosocial choice, no partner
3. R/P: Right lever is the prosocial choice, presence of a partner
4. L/P: Left lever is the prosocial choice, presence of a partner

```{r}
treatment_fct = function(prosoc_lever, partner)
{
	results = rep(-Inf, length = length(prosoc_lever))

	# Right lever is prosocial and...
	results[which(prosoc_lever == 0 & partner == 0)] = 1 # ...no partner
	results[which(prosoc_lever == 0 & partner == 1)] = 3 # ...with partner
	
	# Left lever is prosocial and...
	results[which(prosoc_lever == 1 & partner == 0)] = 2 # ...no partner
	results[which(prosoc_lever == 1 & partner == 1)] = 4 # ...with partner

	return(results)
}

chimpanzees[, treatment := treatment_fct(prosoc_left, condition)]
setkey(x = chimpanzees, actor, block, treatment)
```

#### Mathematical formulation
\begin{equation}
	\begin{aligned}
		L_i &\sim \text{Binomial}(1, p_i) \\
		\logit(p_i) &= \gammai + \alphai + \betai,
	\end{aligned}
\end{equation}
where $\gammai$ is the average odds (more precisely log-odds because on log scale) for each treatment, which is then modified by each actor in each treatment, $\alphai$, and by each block in each treatment, $\betai$.

More specifically, it is possible to rewrite this model with dummy variables, like what would be done in `lme4`. Here is an idea of how it would work in absence of correlation between parameters for the second treatment (hence a 1 at the second position of the vector), chimpanzee 1 (hence the first batch of vector containing the 1, the other values all being 0):

\begin{align*}
	\logit(p_i) &=
		\begin{bmatrix} \gamma_1 & & \\ & \ddots & \\ & & \gamma_4 \end{bmatrix}
		\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} +

		\begin{bmatrix}
			\begin{bmatrix}
				\alpha_{1, 1} & & \\ & \ddots & \\ & & \alpha_{1, 4}
			\end{bmatrix} & & \\
			& \ddots & \\
			& & \begin{bmatrix}
				\alpha_{j, 1} & & \\ & \ddots & \\ & & \alpha_{j, 4}
			\end{bmatrix}
		\end{bmatrix}
		\begin{pmatrix}
			\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} \\
			\vdots \\
			\begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}
		\end{pmatrix} + \\

		& \qquad \begin{bmatrix}
			\begin{bmatrix}
				\beta_{1, 1} & & \\ & \ddots & \\ & & \beta_{1, 4}
			\end{bmatrix} & & \\
			& \ddots & \\
			& & \begin{bmatrix}
				\beta_{j, 1} & & \\ & \ddots & \\ & & \beta_{j, 4}
			\end{bmatrix}
		\end{bmatrix}
		\begin{pmatrix}
			\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} \\
			\vdots \\
			\begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}
		\end{pmatrix} \\
	&= \symbf{\Gamma} \symbf{v_1} + \symbf{A} \symbf{v_2} + \symbf{B} \symbf{v_3}
\end{align*}

The matrix $\symbf{A}$ has a diagonal of `{r} n_chimp*4` elements ($4 \times \text{number chimp}$), and the matrix $\symbf{B}$ has a diagonal of `{r} n_block*4` elements ($4 \times \text{number blocks}$). The unit vectors $v_i, \, 1 \geqslant i \geqslant 3$ have a single non-zero value.

Now, we consider a model with correlations between parameters belonging to a same cluster (here, two clusters: actors and blocks). The vector $v_2$ of `{r} n_block*4` elements becomes an array of `{r} n_chimp` vectors of length 4. Each vector is independent of the others and is drawn from the same multivariate normal distribution, which accounts for the correlation between the elements within each vector. Similarly the vector $v_3$ becomes an array of `{r} n_block` vectors of length 4. These model's details are in the Stan code (@sec-stan_chimp).

#### Stan model, centred parametrisation {#sec-stan_chimp}
```{stan output.var = "model_chimp", cache = TRUE, fold = FALSE}
data {
	// Dimensions
	int <lower = 1> N; // Number of data
	int <lower = 1> N_chimp; // Number of chimpanzees
	int <lower = 1> N_block; // Number of blocks
	int <lower = 1> N_treatment; // Number of treatments
	int <lower = 2*(N_chimp - 1), upper = 2*(N_chimp - 1)> N_measure; // Number of measure per group (actor, block)

	// Explanatory variable
	array[N] int <lower = 1, upper = N_treatment> treatment; // Which treatment is applied for observation i
	vector <lower = 0, upper = 1> [N] left_prosocial; // Boolean 1 if left lever is prosocial, 0 otherwise
	vector <lower = 0, upper = N_block> [N] block_id; // Block id, from 1 to N_block

	// Observations
	array[N] int <lower = 0, upper = 1> left_pull; // Boolean, 1 if left lever pulled, 0 otherwise
}

parameters {
	// Population parameters...
	// ... intercept and slope
	vector[N_treatment] gamma; // Intercept
	array[N_chimp] vector[N_treatment] alpha; // Slope actor
	array[N_block] vector[N_treatment] beta_; // Slope block
	
	// Variances (in the variance-covariance matrices)
	vector<lower = 0> [N_treatment] sigma_diag_actor;
	vector<lower = 0> [N_treatment] sigma_diag_block;
	cholesky_factor_corr[N_treatment] L_actor;
	cholesky_factor_corr[N_treatment] L_block;
}

model {
	real odds = 0;
	int count = 1;
	// Priors...
	// ... variance
	target += lkj_corr_cholesky_lpdf(L_actor | 2);
	target += lkj_corr_cholesky_lpdf(L_block | 2);
	target += exponential_lpdf(sigma_diag_actor | 1);
	target += exponential_lpdf(sigma_diag_block | 1);

	// ... slopes
	target += multi_normal_cholesky_lpdf(alpha | rep_vector(0, N_treatment),
		diag_pre_multiply(sigma_diag_actor, L_actor)); // Vectorised, applied to each vector of the array
	target += multi_normal_cholesky_lpdf(beta_ | rep_vector(0, N_treatment),
		diag_pre_multiply(sigma_diag_block, L_block)); // Vectorised, applied to each vector of the array

	// ... Intercept
	target += normal_lpdf(gamma | 0, 1); // Vectorised, applied to each element of the vector

	// Likelihood and cafe parameters
	for (ch in 1:N_chimp)
	{
		for (bl in 1:N_block)
		{
			for (i in 1:N_measure)
			{
				odds = gamma[treatment[count]] + alpha[ch][treatment[count]] + beta_[bl][treatment[count]];
				target += bernoulli_logit_lpmf(left_pull[count] | odds);
				count = count + 1;
			}
		}
	}
}
```

#### Stan model, non-centred parametrisation {#sec-stan_chimp_nc}
We will see it later, but the centred parametrisation creates divergences, probably due to a funnel-like problem. For the non-centred parametrisation, it is important to remember the following property:

:::{.callout-tip}

## Centred to non-centred parametrisation

Let $X \sim \mvn(\mu, \bSigma)$, then it is equivalent to:

\begin{align*}
	X &= \mu + \symbf{A}Z \\
	Z &\sim \mvn(0, \symbf{\mathds{1}}) \\
	\bSigma &= \symbf{A}\symbf{A}^T,
\end{align*}

where $Z$ is a multivariate normally distributed vector, $\mathds{1}}$ is the identity matrix, and $\symbf{A}$ a matrix. Note that it is then possible to use $\symbf{A} = \diag(\sigma) \symbf{L}$, with $\symbf{L}$ the Cholesky factor.

:::

```{stan output.var = "model_chimp_nc", cache = TRUE, fold = FALSE}
/*
Remarks:
	1. Note that there is no residual variance here! This is because for Bernoulli variables, B(p), the odds are p,
		and the variance is p(1 - p). So only one parameter!
	2. The non-centred parametrisation
*/

data {
	// Dimensions
	int <lower = 1> N; // Number of data
	int <lower = 1> N_chimp; // Number of chimpanzees
	int <lower = 1> N_block; // Number of blocks
	int <lower = 1> N_treatment; // Number of treatments
	int <lower = 2*(N_chimp - 1), upper = 2*(N_chimp - 1)> N_measure; // Number of measure per group (actor, block)

	// Explanatory variable
	array[N] int <lower = 1, upper = N_treatment> treatment; // Which treatment is applied for observation i
	vector <lower = 0, upper = 1> [N] left_prosocial; // Boolean 1 if left lever is prosocial, 0 otherwise
	vector <lower = 0, upper = N_block> [N] block_id; // Block id, from 1 to N_block

	// Observations
	array[N] int <lower = 0, upper = 1> left_pull; // Boolean, 1 if left lever pulled, 0 otherwise
}

parameters {
	// Population parameters...
	// ... intercept and slopes
	vector[N_treatment] gamma; // Intercept
	array[N_chimp] vector[N_treatment] Z_chimp; // Actor effect
	array[N_block] vector[N_treatment] Z_block; // Block effect
	
	// Variances (in the variance-covariance matrices)
	vector<lower = 0> [N_treatment] sigma_diag_actor;
	vector<lower = 0> [N_treatment] sigma_diag_block;
	cholesky_factor_corr[N_treatment] L_actor;
	cholesky_factor_corr[N_treatment] L_block;
}

transformed parameters {
	array[N_chimp] vector[N_treatment] alpha; // Slope actor
	array[N_block] vector[N_treatment] beta_; // Slope block

	for (ch in 1:N_chimp)
		alpha[ch] = diag_pre_multiply(sigma_diag_actor, L_actor) * Z_chimp[ch];
	
	for (bl in 1:N_block)
		beta_[bl] = diag_pre_multiply(sigma_diag_block, L_block) * Z_block[bl];
}

model {
	real odds = 0;
	int count = 1;
	// Priors...
	// ... variance
	target += lkj_corr_cholesky_lpdf(L_actor | 2);
	target += lkj_corr_cholesky_lpdf(L_block | 2);
	target += exponential_lpdf(sigma_diag_actor | 1);
	target += exponential_lpdf(sigma_diag_block | 1);

	// ... slopes
	target += multi_normal_lpdf(Z_chimp | rep_vector(0, N_treatment), identity_matrix(N_treatment));
	target += multi_normal_lpdf(Z_block | rep_vector(0, N_treatment), identity_matrix(N_treatment));

	// ... Intercept
	target += normal_lpdf(gamma | 0, 1); // Vectorised, applied to each element of the vector

	// Likelihood and cafe parameters
	for (ch in 1:N_chimp)
	{
		for (bl in 1:N_block)
		{
			for (i in 1:N_measure)
			{
				odds = gamma[treatment[count]] + alpha[ch][treatment[count]] + beta_[bl][treatment[count]];
				target += bernoulli_logit_lpmf(left_pull[count] | odds);
				count = count + 1;
			}
		}
	}
}

generated quantities {
	cov_matrix[N_treatment] Sigma_actor;
	cov_matrix[N_treatment] Sigma_block;
	vector[N] new_sim;
	
	// This is to recover the variance-covariance matrix instead of having its Cholesky factor
	Sigma_actor = multiply_lower_tri_self_transpose(diag_pre_multiply(sigma_diag_actor, L_actor));
	Sigma_block = multiply_lower_tri_self_transpose(diag_pre_multiply(sigma_diag_block, L_block));

	{
		int count = 1;
		real odds = 0;

		for (ch in 1:N_chimp)
		{
			for (bl in 1:N_block)
			{
				for (i in 1:N_measure)
				{
					odds = gamma[treatment[count]] + alpha[ch][treatment[count]] + beta_[bl][treatment[count]];
					new_sim[count] = bernoulli_logit_rng(odds);
					count = count + 1;
				}
			}
		}
	}
}
```

#### Stan data
```{r}
## Prepare data
stanData = list(
	N = chimpanzees[, .N],
	N_chimp = n_chimp,
	N_block = n_block,
	N_treatment = 4,
	N_measure = 2*(n_chimp - 1),

	treatment = chimpanzees[, treatment],
	left_prosocial = chimpanzees[, prosoc_left],
	block_id = chimpanzees[, block],

	left_pull = chimpanzees[, pulled_left]
)

# model_chimp = cmdstan_model("./04_test.stan")
# model_chimp_nc = cmdstan_model("./04_test_bis.stan")

## Common variables
n_chains = 4

iter_warmup = 1000
iter_sampling = 1000
```

#### Run the model
```{r}
#| output: false
#| fold: false
## Run
fit = model_chimp$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)

fit_nc = model_chimp_nc$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)
```

### Results {#sec-results_chimp}

```{r}
gamma_est = round(apply(X = fit_nc$draws("gamma", inc_warmup = FALSE), MARGIN = 3, FUN = mean), 2)
sigma_act_est = round(apply(X = fit_nc$draws("sigma_diag_actor", inc_warmup = FALSE), MARGIN = 3, FUN = mean), 2)
sigma_blo_est = round(apply(X = fit_nc$draws("sigma_diag_block", inc_warmup = FALSE), MARGIN = 3, FUN = mean), 2)
```

```{r}
lazyPosterior(fit_nc$draws("gamma[1]", inc_warmup = FALSE), fun = dnorm, mean = 0, sd = 1,
	xlab = "gamma[1]")
lazyPosterior(fit_nc$draws("sigma_diag_actor[1]", inc_warmup = FALSE), fun = dexp, rate = 1,
	xlab = "sigma_diag_actor[1]")
```

Before finishing this example, let us check the average posterior predictions for the block 5, as done in @McElreath2020 (p. 451-452). For this, I use the generated quantity `new_sim`:

```{r}
ind_bl_5 = chimpanzees[, which(block == 5)]
n_measure = 2*(n_chimp - 1)

dt = data.table(chimp = rep(1:n_chimp, each = 4), treatment = rep(1:4, n_chimp), avg = rep(-Inf, 4*n_chimp),
	pch = rep(c(1, 1, 19, 19), n_chimp), key = c("chimp", "treatment"))

posterior_sim = fit_nc$draws("new_sim", inc_warmup = FALSE)[, , ind_bl_5]

for (i in seq_len(n_chimp))
{
	for (tt in 1:4)
	{
		ind = chimpanzees[ind_bl_5][.(i), which(treatment == tt)] + (i - 1)*n_measure
		dt[.(i, tt), avg := mean(posterior_sim[, , ind])]
	}
}

plot(1:dt[, .N], dt[, avg], axes = FALSE, xlab = "", ylab = "Prop. left lever", ylim = c(0, 1),
	pch = dt[, pch])
abline(h = 0.5, col = "#11223322")
for (i in 2:n_chimp - 1)
	abline(v = 4*i + 0.5, lty = "dashed", col = "#11223322")
segments(x0 = seq(1, dt[, .N], by = 4), y0 = dt[seq(1, .N, by = 4), avg],
	x1 = seq(3, dt[, .N], by = 4), y1 = dt[seq(3, .N, by = 4), avg])
segments(x0 = seq(2, dt[, .N], by = 4), y0 = dt[seq(2, .N, by = 4), avg],
	x1 = seq(4, dt[, .N], by = 4), y1 = dt[seq(4, .N, by = 4), avg])
axis(1)
axis(2, las = 1, at = c(0, 0.5, 1))
```

This ends the Chimpanzee example. There is a third example about dyad in households (pairs of households that exchange gifts), but I will not do it now, as it does not bring much more stuff.

However, the next example in @McElreath2020 (p. 468) talks about continuous categories and the Gaussian process. So far, the examples only had discrete variables such as cafes, block, chimpanzee. But what about continuous variables such as age or income? People of the same age for instance experienced many things in common (music, political events, ...). People of *similar* age (\ie close but not equal) were also exposed to similar stuff (hence the term generation, which span over few years) although to a lesser degree than people of the same age. It would not make sense to estimate a unique intercept for individuals of the same age and ignoring the fact that people of similar age should have more similar intercepts. The general approach is known as **Gaussian process regression** and is the topic of the next example

## Spatial autocorrelation in Oceanic tools

### Data description {#sec-data_ocean}
The dataset `Kline` (renamed `tool_dt`) contains 5 columns:

1. culture: name of the population
2. population: historical size of the population
3. contact: the level of contact of a population to its 'outside' world
4. total_tools: the number of unique tools
5. mean_TU: a measure of tool complexity

```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

source("./toolFunctions.R")

data(Kline)
setDT(Kline)
tool_dt = Kline

data(islandsDistMatrix)
dist_mat = islandsDistMatrix

rm(Kline, islandsDistMatrix)

coords = data.table(island = c("Malekula", "Tikopia", "Santa Cruz", "Yap", "Lau Fiji",
	"Trobriand", "Chuuk", "Manus", "Tonga", "Hawaii"),
	lat = c(-16.3, -12.3, -10.7, 9.5, -17.7, -8.7, 7.4, -2.1, -21.2, 19.9),
	lon = c(167.5, 168.3, 165.8, 138.1, -179.1, 151.0, 151.8, 146.9, -175.2, -155.5)
)

coords = project(x = vect(x = coords, crs = "epsg:4326"), y = "EPSG:9191")

oceania = project(x = vect("~/work/database/shapefile/oceania/dt465jv7171.shp"), y = "EPSG:9191")
zone = ext(oceania)
zone$ymin = -2.5e6
oceania = crop(oceania, zone)
```

This dataset takes place in historical island societies of Oceania (Hawai, Fiji, Santa Cruz to give the most known islands, see @fig-map_islands). It gathers the amout of unique tools (boats, axes, fish hooks, ...) each population had. A theory of technological evolution predicts that larger population will both develop and sustain more complex tool kits. Then, biogeography states that more connected and larger islands are more likely to have larger populations.

```{r}
#| echo: false
#| label: fig-map_islands
#| fig-cap: "Maps of the island locations, next to Australia"
plot(oceania, axes = FALSE, lwd = 1, col = "#112233")
points(coords, pch = 19, col = "#CD212A")
text(coords, labels = coords[["island"]][, 1], pos = 4)
```

### Fit the model {#sec-fit_ocean}

In this example, we are interested into predicting the number of tools in function of contact rate and population size. We use a Poisson distribution for this, as they are counts data.

#### Mathematical formulation
The model that follows is justified in @McElreath2020 (p. 355-356). It represents the amount of tools in a population given the size of the population. Note that we get for free a 0 tool when there is a population of 0 (provided that $\beta > 0$)!
\begin{align*}
	T_i &\sim \poiss(\lambda_i) \\
	\log(\lambda_i) &= \alpha \frac{P_i^{\beta}}{\gamma}
\end{align*}
where $\alpha$ is the amount of tools brought by a person, $\beta$ is the rate of tools brought by a person. We expect this rate to be between 0 and 1, that is to say, each additional person adds less to innovation than the previous; $\gamma$ is the rate of tool loss (provided $\gamma > 1$).


#### Stan model, non-centred parametrisation {#sec-stan_ocean_nc}
```{stan output.var = "model_ocean", cache = TRUE, fold = FALSE}
// data {
// 	// Dimensions


// 	// Explanatory variable

// 	// Observations

// }

// parameters {

// }

// transformed parameters {

// }

// model {

// }

// generated quantities {
// }
```


#### Stan data
```{r}
## Prepare data
stanData = list(

)

## Common variables
n_chains = 4

iter_warmup = 1000
iter_sampling = 1000
```

#### Run the model
```{r}
#| output: false
#| fold: false
## Run
# fit = model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
# 	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
# 	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)
```

### Results {#sec-results_ocean}

## Discussion on where to put the correlation for my own case

```{r}
#| output: false
#| echo: false

#### Load the data
## Path
os = Sys.info()[['sysname']]
mnt_point = "/mnt/local_share/"
if (os == "Linux" || os == "Darwin")
{
	if (!dir.exists(mnt_point))
		stop(paste0("The mounting point <", mnt_point, "> does not exist"))
} else if (os == "Windows") {
	stop("TO DO!!! No idea how that works on Windows!")
} else {
	stop(paste("Unknown Operating System:", os))
}

path_data = paste0(mnt_point, "/data/")
if (!dir.exists(path_data))
	stop(paste0("Folder <", path_data, "> does not exist! Did you run 01_prepare_data.qmd?"))

opt = "_full"

## Loading
stanData = readRDS(paste0(path_data, "stanData_nfi-inra", opt, ".rds"))
forest_dt = readRDS(paste0(path_data, "nfi-inra_dt", opt, ".rds"))
ind_species = readRDS(paste0(path_data, "ind_species_nfi-inra", opt, ".rds"))

colours = paste0(met.brewer("Egypt")[1:4])
names(colours) = c("conifer", "broadleaf", "inra", "nfi")

forest_dt[fct_type == "broadleaf", fct_colour := colours["broadleaf"]]
forest_dt[fct_type == "conifer", fct_colour := colours["conifer"]]
forest_dt[origin_data == "inra", origin_colour := colours["inra"]]
forest_dt[origin_data == "nfi", origin_colour := colours["nfi"]]

## Add vernacular english names
# vernacularNames = sci2comm(sci = forest_dt[, unique(speciesName_sci)], db = "itis")
```

I have about \num{10000} felled trees, on which the bole volume, $V_b$, and crown volume, $V_c$, have been measured. The two volumes have a correlation of `{r} round(cor(forest_dt["broadleaf"][origin_data == "inra", bole_volume_m3], forest_dt["broadleaf"][origin_data == "inra", crown_volume_m3]), 3)` for broadleaves and of `{r} round(cor(forest_dt["conifer"][origin_data == "inra", bole_volume_m3], forest_dt["conifer"][origin_data == "inra", crown_volume_m3]), 3)` for conifers(see @fig-correl):

```{r}
#| echo: false
#| label: fig-correl
#| fig-cap: Correlation between the bole volume and the crown volume
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

plot(forest_dt[origin_data == "inra", bole_volume_m3], forest_dt[origin_data == "inra", crown_volume_m3],
	xlab = "Bole volume (mÂ³)", ylab = "Crown volume (mÂ³)", axes = FALSE, pch = 19,
	col = paste0(forest_dt[origin_data == "inra", fct_colour], "33"))

axis(1)
axis(2, las = 1)
legend(x = "topright", legend = c("Conifer", "Broadleaf"), fill = colours[c("conifer", "broadleaf")], bty = "n")

```

However, is it truly the case, or is it a correlation coming from a confounder controlling both volumes, such as diameter (see @fig-correl_scheme for a graphical representation)? The solution is to test for conditional independencies on $\phi$: $[V_b | \phi]$ and $[V_c | \phi]$. In other words, does all the information that $V_b$ brings to $V_c$ come from $\phi$? For this, I do a linear regression (see @McElreath2020, chapters 5 and 6, especially section 6.4).

![Possibility of spurious correlations](img/spurious.tex){#fig-correl_scheme}

```{r}
cor(forest_dt["broadleaf"][origin_data == "inra", height],
	forest_dt["broadleaf"][origin_data == "inra", circumference_m])
cor(forest_dt["conifer"][origin_data == "inra", height],
	forest_dt["conifer"][origin_data == "inra", circumference_m])

lm_broadleaf = lm(crown_volume_m3 ~ bole_volume_m3 + circumference_m + height,
	forest_dt["broadleaf"][origin_data == "inra"])
lm_conifer = lm(crown_volume_m3 ~ bole_volume_m3 + circumference_m + height,
	forest_dt["conifer"][origin_data == "inra"])

dt = data.table(c = runif(1e3, 0, 100))
dt[, x := rnorm(.N, 2*c + 5, 1.3)]
dt[, y := rnorm(.N, 1.2*c - 4, 0.12)]

cor(dt[, x], dt[, y])

l1 = lm(y ~ x, dt)
l2 = lm(y ~ x + c, dt)

jtools::plot_summs(l1)
jtools::plot_summs(l2)

jtools::plot_summs(lm_broadleaf)
jtools::plot_summs(lm_conifer)
```

This is a very rough test as there is clearly no homoscedasticity, however, the bole volume is actually much more important than expected!

```{r}
library(dagitty)

dag = dagitty("dag{
	V_b -> V_c
	V_b <- phi -> V_c
	V_b <- h -> V_c
	V_b <- phi -> h -> V_c
}")

plot(graphLayout(dag))
adjustmentSets(dag, exposure = "V_b", outcome = "V_c")
impliedConditionalIndependencies(dag)
```

This lead to the investigation and discussion on the form coefficient (see `05_formFactor.qmd`) as it is supposed to reduce heteroscedasticity.

### Form ratio (Facteur de forme)

```{r}
#| echo: false
#| output: false

# forest_dt[, formFactor := 4*pi*bole_volume_m3/(height*circumference_m^2)*(1 - 1.3/height)^2]
# forest_dt[origin_data == "nfi", origin_colour := paste0(origin_colour, "22")]
# forest_dt[origin_data == "nfi", pt_size := 0.5]
# forest_dt[origin_data == "inra", pt_size := 1]

# for (sp in unique(forest_dt[, speciesName_sci]))
# {
# 	png(paste0("output/", sp, ".png"), width = 1080, height = 1080)
# 	layout(matrix(c(1:4), ncol = 2, nrow = 2))
# 	plot(forest_dt[speciesName_sci == sp, height], forest_dt[speciesName_sci == sp, formFactor],
# 		xlab = "Height", ylab = "Form factor", axes = FALSE, pch = 3,
# 		cex = forest_dt[speciesName_sci == sp, pt_size],
# 		col = forest_dt[speciesName_sci == sp, origin_colour])
# 	axis(1)
# 	axis(2, las = 1)

# 	plot(forest_dt[speciesName_sci == sp, circumference_m], forest_dt[speciesName_sci == sp, formFactor],
# 		xlab = "Circumference", ylab = "Form factor", axes = FALSE, pch = 3,
# 		cex = forest_dt[speciesName_sci == sp, pt_size],
# 		col = forest_dt[speciesName_sci == sp, origin_colour])
# 	axis(1)
# 	axis(2, las = 1)

# 	plot(forest_dt[speciesName_sci == sp, height], forest_dt[speciesName_sci == sp, bole_volume_m3],
# 		xlab = "Height", ylab = "Form factor", axes = FALSE, pch = 3,
# 		cex = forest_dt[speciesName_sci == sp, pt_size],
# 		col = forest_dt[speciesName_sci == sp, origin_colour])
# 	axis(1)
# 	axis(2, las = 1)

# 	plot(forest_dt[speciesName_sci == sp, circumference_m], forest_dt[speciesName_sci == sp, bole_volume_m3],
# 		xlab = "Circumference", ylab = "Form factor", axes = FALSE, pch = 3,
# 		cex = forest_dt[speciesName_sci == sp, pt_size],
# 		col = forest_dt[speciesName_sci == sp, origin_colour])
# 	axis(1)
# 	axis(2, las = 1)
# 	dev.off()
# }

# aa = forest_dt[speciesName_sci == "Quercus pubescens"]

# cor(aa$formFactor, aa$height)
# cor(aa$formFactor, aa$circumference_m)

```

## Exploration of the model from @Zhou2021

@Zhou2021 relates the above-ground biomass (defined as branches, bark, leaves, and fruits), AGB, to the stem biomass (main trunk, from ground to tip I think), SB. They developed the following formula for indvidual trees (equation 4 in @Zhou2021):
\begin{align*}
	y &= \frac{L \phi^2 h}{m - d \exp[-k x]} \\
	L &= \frac{\pi}{4}w f,
\end{align*}
where $y$ is the AGB, $\phi$ and $h$ are the dbh and height, $m$, $d$, and $k$ are estimated parameters, $w$ is the wood density, and $f$ the form factor defined as:
$$
f = \frac{4V}{\pi \phi^2 h},
$$
with $V$ the total volume of the stem. Then, we get another expression of $y$:
\begin{align*}
	y &= \frac{4 V}{\pi(m - d \exp[-k x])} \\
		&= \frac{V}{m' - d' \exp[-k x]}
\end{align*}
where $m' = \pi m/4$ and $d' = \pi d/4$. Let us try this on the Emerge data!

### Test of the formula from @Zhou2021 on Emerge data

```{r}
#| echo: false
#| output: false

## Subset
forest_dt = forest_dt[origin_data == "inra"]
species_vallet = c("Abies alba", "Fagus sylvatica", "Picea abies", "Pinus pinaster", "Pinus sylvestris",
	"Pseudotsuga menziesii", "Quercus petraea")
forest_dt = forest_dt[speciesName_sci %in% species_vallet]

## New variables
forest_dt[, hdn := sqrt(circumference_m)/height]
forest_dt[, slenderness := pi*height/circumference_m] # height/diameter
forest_dt[, formBFT := 4*pi*bole_volume_m3/(height*circumference_m^2)*(1 - 1.3/height)^2]
forest_dt[, formTot_vallet := 4*pi*total_volume_m3/(height*circumference_m^2)]
forest_dt[, form_zhou := 4*pi*bole_volume_m3/(height*circumference_m^2)]

## Add wood density
wood_dens = fread("../data/xylo-dens-map/xdm_species_density_data.csv")[, .(Species, Mean)]
setnames(wood_dens, new = c("speciesName_sci", "w"))
forest_dt = merge.data.table(forest_dt, wood_dens, by = "speciesName_sci")

## Set keys
setkey(forest_dt, speciesName_sci)
n_sp = length(forest_dt[, unique(speciesName_sci)])

## Picea and abies
m = 0.893
d = 0.378
k = -0.0027 # Use the version for equation 4 and not 5

range(forest_dt[genus %in% c("Picea", "Abies"), form_zhou])
plot(density(forest_dt[genus %in% c("Picea", "Abies"), form_zhou]), lwd = 3, main = "", xlab = "Form factor (Zhou)", axes = FALSE)
axis(1)
axis(2, las = 1)

forest_dt[genus %in% c("Picea", "Abies"), zhou_vol := bole_volume_m3/(m - d*exp(-k*circumference_m^2*height/pi^2))]

# ## Log, degree 2
# plot(forest_dt[genus %in% c("Picea", "Abies"), log(zhou_vol)],
# 	forest_dt[genus %in% c("Picea", "Abies"), log(total_volume_m3)],
# 	pch = 19, cex = 0.5, xlab = "Log est. total volume (Zhou)", ylab = "Log measured total volume",
# 	xlim = c(min(forest_dt[genus %in% c("Picea", "Abies"), .(log(zhou_vol), log(total_volume_m3))]),
# 		max(forest_dt[genus %in% c("Picea", "Abies"), .(log(zhou_vol), log(total_volume_m3))])),
# 	ylim = c(min(forest_dt[genus %in% c("Picea", "Abies"), .(log(zhou_vol), log(total_volume_m3))]),
# 		max(forest_dt[genus %in% c("Picea", "Abies"), .(log(zhou_vol), log(total_volume_m3))])))

# tt = lm(formula = log(total_volume_m3) ~ 1 + log(zhou_vol) + I(log(zhou_vol)^2), data = forest_dt[genus %in% c("Picea", "Abies")])

# cof = coef(tt)
# ff = function(x) {return(cof[1] + cof[2]*x + cof[3]*x^2)}
# curve(ff, lwd = 3, col = "#CD211A", add = TRUE)
# curve(ff, lwd = 3, col = "#3355AA", add = TRUE)

# sim = simulateResiduals(fittedModel = tt, plot = FALSE)
# plotResiduals(sim, quantreg = TRUE)
# plotResiduals(sim, form = forest_dt[genus %in% c("Picea", "Abies"), log(zhou_vol)])

## Log, degree 4 #dd5129 #0f7ba2 #43b284 #fab255 
forest_dt[genus == "Picea", col_sp := "#FAB255"]
forest_dt[genus == "Abies", col_sp := "#43B284"]
plot(forest_dt[genus %in% c("Picea", "Abies"), log(zhou_vol)],
	forest_dt[genus %in% c("Picea", "Abies"), log(total_volume_m3)], pch = 19, cex = 0.5,
	xlab = "Est. total volume (Zhou)", ylab = "Measured total volume",
	col = forest_dt[genus %in% c("Picea", "Abies"), col_sp])

# points(forest_dt[genus %in% c("Picea", "Abies"), log(zhou_vol)],
# 	forest_dt[genus %in% c("Picea", "Abies"), log(bole_volume_m3)], pch = 19, cex = 0.5, col = "#FAB255")
abline(a = 0, b = 1, lwd = 2, col = "#CD212A")
tt2 = lm(formula = log(total_volume_m3) ~ speciesName_sci + (log(zhou_vol) + I(log(zhou_vol)^2) +
	I(log(zhou_vol)^3) + I(log(zhou_vol)^4)):speciesName_sci,
	data = forest_dt[genus %in% c("Picea", "Abies")])
cof_abies = coef(tt2)[seq(1, 9, 2)]
cof_picea = c(sum(coef(tt2)[1:2]), coef(tt2)[seq(4, 10, 2)])
ff = function(x, cof) {return(cof[1] + cof[2]*x + cof[3]*x^2 + cof[4]*x^3 + cof[5]*x^4)}
curve(ff(x, cof_abies), lwd = 3, col = "#3355AA", add = TRUE)
curve(ff(x, cof_picea), lwd = 3, col = "#AA5533", add = TRUE)

sim = simulateResiduals(fittedModel = tt2, plot = FALSE)
plotResiduals(sim, quantreg = TRUE)
plot(sim)

## Non-log, degree 4
plot(forest_dt[genus %in% c("Picea", "Abies"), zhou_vol],
	forest_dt[genus %in% c("Picea", "Abies"), total_volume_m3], pch = 19, cex = 0.5,
	xlab = "Est. total volume (Zhou)", ylab = "Measured total volume")

# # Deg 4
# tt2 = lm(formula = total_volume_m3 ~ 0 + zhou_vol + I(zhou_vol^2) + I(zhou_vol^3) + I(zhou_vol^4),
# 	data = forest_dt[genus %in% c("Picea", "Abies")])
# cof = coef(tt2)
# ff = function(x) {return(cof[1]*x + cof[2]*x^2 + cof[3]*x^3 + cof[4]*x^4)}
# curve(ff, lwd = 3, col = "#3355AA", add = TRUE)

# # Deg 2
# tt2 = lm(formula = total_volume_m3 ~ 0 + zhou_vol + I(zhou_vol^2),
# 	data = forest_dt[genus %in% c("Picea", "Abies")])
# cof = coef(tt2)
# ff = function(x) {return(cof[1]*x + cof[2]*x^2)}
# curve(ff, lwd = 2, col = "#CD212A", add = TRUE)

# # Deg 1
# tt2 = lm(formula = total_volume_m3 ~ 0 + zhou_vol, data = forest_dt[genus %in% c("Picea", "Abies")])
# abline(tt2, lwd = 2, col = "#FAB255")

# sim = simulateResiduals(fittedModel = tt2, plot = FALSE)
# plotResiduals(sim, quantreg = TRUE)

# ## Volume houppier
# forest_dt[, v_h := total_volume_m3 - bole_volume_m3]
# plot(forest_dt[, bole_volume_m3],forest_dt[, v_h], pch = 19, cex = 0.5)
# abline(a = 0, b = 1)

# ## Ratio bole volume et c^2h (prop to cylindre vol)
# forest_dt[, ratio := bole_volume_m3/(circumference_m^2*height)]
# plot(forest_dt[, log(ratio)], forest_dt[, log(total_volume_m3)], pch = 19, cex = 0.5)

# ## Plot Zhou's ratio
# plot(forest_dt[, bole_volume_m3], forest_dt[, 1/(m - d*exp(-k*circumference_m^2*height/pi^2))], pch = 19, cex = 0.5)

# WFO.remember(WFO.file = "~/work/database/worldFlora/classification.csv")
# WFO.match(spec.data = "Larix decidua", WFO.data = WFO.data)
# comm2sci(com = "Norway spruce", db = "itis")


aa_log = 0.8 + 1.2*log(3) + 2.8*log(9)
exp(aa_log) - (exp(0.8)*3^1.2*3^5.6)
exp(aa_log) - (exp(0.8)*3^6.8)
```

### Create the model
#### Prior check
```{r}
set.seed(1969 - 08 - 18) # Woodstock seed

n_draws = 200
n_trees = 1e3

dt = forest_dt[sample(x = 1:.N, size = n_trees, replace = FALSE),
	.(circumference_m, height, bole_volume_m3, total_volume_m3)]
ll = vector(mode = "list", length = n_draws)
params = vector(mode = "list", length = n_draws)

for (i in seq_len(n_draws))
{
	m = rnorm(1, mean = 0.9, sd = 0.05);
	d = rnorm(1, mean = 0.3, sd = 0.05);
	k = rnorm(1, mean = 0,  sd =0.01);

	sigma = rgamma(1, shape = 2.5, rate = 7);

	ll[[i]] = rnorm(n_trees, mean = dt[, log(bole_volume_m3)]/(m - d*exp(k*dt[, circumference_m]^2*dt[, height])), sigma);
	params[[i]] = data.table(m = m, d = d, k = k, sigma = sigma)
}

params = rbindlist(params)
# tt = rbindlist(ll)

params[k < 0][which.max(sigma)]
qq = params[, which((-0.01044 < k) & (k < -0.01043))]

plot(dt[, bole_volume_m3], exp(ll[[qq]]), pch = 19, cex = 0.5, xlab = "bole volume", ylab = "pred tot volume") # Show that sigma should be small

plot(dt[, bole_volume_m3], exp(ll[[198]]), pch = 19, cex = 0.5, xlab = "bole volume", ylab = "pred tot volume") # Shos that  k > 0 lead to big values

plot(dt[, log(bole_volume_m3)], ll[[200]], pch = 19, cex = 0.5, xlab = "log(bole volume)", ylab = "log(pred tot volume)") # Show a coherent result

plot(dt[, bole_volume_m3], dt[, total_volume_m3], col = "#CD212A", pch = 19, cex = 0.3)
abline(a = 0, b = 1, lwd = 2)
plot(dt[, log(bole_volume_m3)], dt[, log(total_volume_m3)], col = "#CD212A", pch = 19, cex = 0.3)
abline(a = 0, b = 1, lwd = 2)
```

#### model
```{r}
#### Fit the model
## Compile model
model = cmdstan_model("./04_zhou.stan")

## Stan data
stanData = list(
	N = forest_dt[genus %in% c("Picea", "Abies"), .N],
	S = forest_dt[genus %in% c("Picea", "Abies"), length(unique(speciesName_sci))],
	start = forest_dt[genus %in% c("Picea", "Abies")][, .(start = .I[1]), by = .(speciesName_sci)][, start],
	end = forest_dt[genus %in% c("Picea", "Abies")][, .(end = .I[.N]), by = .(speciesName_sci)][, end],

	circumference = forest_dt[genus %in% c("Picea", "Abies"), circumference_m],
	height = forest_dt[genus %in% c("Picea", "Abies"), height],
	bole_volume = forest_dt[genus %in% c("Picea", "Abies"), bole_volume_m3],

	tot_volume = forest_dt[genus %in% c("Picea", "Abies"), total_volume_m3]
)

## Common variables
n_chains = 4

iter_warmup = 250
iter_sampling = 250
```

### Run the model
```{r}
#| output: false
#| fold: false
## Run
fit = model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 50, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)

fit$save_output_files(dir = "./", basename = paste0("fit_zhou"), random = FALSE)
saveRDS(fit, "./fit_zhou.rds")
```

### Posterior check
#### Check some values
```{r}
apply(X = fit$draws("beta0"), MARGIN = 3, FUN = mean)
apply(X = fit$draws("beta1"), MARGIN = 3, FUN = mean)

round(apply(X = fit$draws("m"), MARGIN = 3, FUN = mean), 3)
round(apply(X = fit$draws("k"), MARGIN = 3, FUN = mean), 3)
round(apply(X = fit$draws("d"), MARGIN = 3, FUN = mean), 3)

round(apply(X = fit$draws("sigma"), MARGIN = 3, FUN = mean), 3)
```

#### Generate posterior simulations
```{r}
genQ = cmdstan_model("./04_zhou_gq.stan")

stanData_new = stanData

stanData_new[["selected_species"]] = seq_along(species_vallet)
selected_species = species_vallet[stanData_new[["selected_species"]]]
new_forest = forest_dt[.(selected_species)]

stanData_new[["S_new"]] = length(stanData_new[["selected_species"]])
stanData_new[["N_new"]] = new_forest[, .N]
stanData_new[["start_new"]] = new_forest[, .(start = .I[1]), by = .(speciesName_sci)][, start]
stanData_new[["end_new"]] = new_forest[, .(end = .I[.N]), by = .(speciesName_sci)][, end]

stanData_new[["circumference_new"]] = new_forest[, circumference_m]
stanData_new[["height_new"]] = new_forest[, height]
stanData_new[["bole_volume_new"]] = new_forest[, bole_volume_m3]

sim = genQ$generate_quantities(fit$draws(inc_warmup = FALSE), data = stanData_new)
```

```{r}
mean_pred = apply(X = sim$draws("mean_pred"), MARGIN = 3, FUN = mean)
ratio = apply(X = sim$draws("ratio"), MARGIN = 3, FUN = mean)
rm(sim)

plot(new_forest[, total_volume_m3], ratio, pch = 19, cex = 0.5)

## Plot prediction vs measured
plot(new_forest[, total_volume_m3], exp(mean_pred), pch = 19, cex = 0.5,
	xlab = "Measured tot. vol.", ylab = "Pred. tot. vol.")
abline(a = 0, b = 1, lwd = 2, col = "#CD212A")

## Plot prediction vs measured in log space
plot(new_forest[, log(total_volume_m3)], mean_pred, pch = 19, cex = 0.5,
	xlab = "log(measured tot. vol.)", ylab = "log(pred. tot. vol.)")
abline(a = 0, b = 1, lwd = 2, col = "#CD212A")

# ## Plot simple residuals
# plot(new_forest[, total_volume_m3] - exp(mean_pred), ylab = "Residuals (real space)", pch = 19, cex = 0.5)

# ## Plot simple residuals in log space
# plot(new_forest[, log(total_volume_m3)] - mean_pred, ylab = "Residuals (log space)", pch = 19, cex = 0.5)

## Standardised residuals
plot(mean_pred, sqrt(abs(new_forest[, log(total_volume_m3)] - mean_pred)), pch = 19, cex = 0.5,
	ylab = "sqrt(std. res.)")

## Residuals vs observations
plot(new_forest[, total_volume_m3], new_forest[, total_volume_m3] - exp(mean_pred), pch = 19, cex = 0.5,
	xlab = "Obs. tot. vol", ylab = "Residuals")
```

#### Residuals using DHARMa
```{r}
library(DHARMa)

n_sampling = fit$metadata()$iter_sampling

simResp = sim$draws("pred_tot_volume")
dim(simResp)

# Reshape simResp as a matrix nobs x n_repetition
simResp_matrix = matrix(data = 0, nrow = new_forest[, .N], ncol = n_sampling*n_chains)
for (obs in seq_len(new_forest[, .N]))
{
	col_start = 1
	for (chain in 1:n_chains)
	{
		col_end = chain*n_sampling
		simResp_matrix[obs, col_start:col_end] = simResp[, chain, obs]
		col_start = col_end + 1
	}
}

## Simulated residuals
sim_dharma = createDHARMa(simulatedResponse = simResp_matrix,
	observedResponse = new_forest[, total_volume_m3],
	fittedPredictedResponse = apply(sim$draws("mean_pred"), MARGIN = 3, median),
	integerResponse = FALSE)

plot(sim_dharma, quantreg = TRUE)
plotResiduals(sim_dharma, new_forest[, bole_volume_m3], quantreg = TRUE)

ind_min = which.min(new_forest[, bole_volume_m3])
ind_max = which.max(new_forest[, bole_volume_m3])

dd = density(simResp[, , ind_min])
plot(dd, xlab = "Simulated volume (min predictor)", xlim = c(0, max(dd$x, new_forest[ind_min, total_volume_m3])),
	main = paste("Real value:", round(new_forest[ind_min, total_volume_m3], 3)))
abline(v = new_forest[ind_min, total_volume_m3], lwd = 3, col = "#CD212A")

dd = density(simResp[, , 9])
plot(dd, xlab = "Simulated volume (min predictor)", xlim = c(0, max(dd$x, new_forest[9, total_volume_m3])),
	main = paste("Real value:", round(new_forest[9, total_volume_m3], 3)))
abline(v = new_forest[9, total_volume_m3], lwd = 3, col = "#CD212A")

zz = (new_forest[, bole_volume_m3] - new_forest[, min(bole_volume_m3)])/diff(range(new_forest[, bole_volume_m3]))
zz[5630]
```


```{r}
aa = forest_dt[genus %in% c("Picea", "Abies")]

plot(aa[, circumference_m], aa[, total_volume_m3], pch = 19, cex = 0.5)
plot(aa[, circumference_m], aa[, bole_volume_m3], pch = 19, cex = 0.5)
plot(aa[, height], aa[, bole_volume_m3], pch = 19, cex = 0.5)
plot(aa[, hdn], aa[, bole_volume_m3], pch = 19, cex = 0.5)
plot(aa[, circumference_m^2*height/(4*pi)], aa[, total_volume_m3], pch = 19, cex = 0.5)
plot(aa[, circumference_m^2*height/(4*pi)], aa[, total_volume_m3], pch = 19, cex = 0.5)



library(interp)

aa <- aa[!duplicated(cbind(aa$circumference_m, aa$height))]
aa = aa[sample(x = seq_len(.N), size = 500, replace = FALSE)]


si <- interp(x = aa[, circumference_m], y = aa[, height], z = aa[, bole_volume_m3],
	method = "linear", duplicate = "mean")

filled.contour(si,
	plot.axes = {
		axis(1)
		axis(2, las = 1)
		contour(si, add = TRUE, lwd = 2, axes = NULL)
	}
)




si <- interp(x = aa[, circumference_m], y = aa[, height], z = aa[, bole_volume_m3],
	method = "linear", duplicate = "mean")

filled.contour(si,
	plot.axes = {
		axis(1)
		axis(2, las = 1)
		contour(si, add = TRUE, lwd = 2, axes = NULL)
	}
)
```

## Exercises from @McElreath2020

### 6M2, p. 189

```{r}
# Consider the DAG X â Z â Y. Simulate data from this DAG so that the correlation between X and Z is very large.
# 	Then include both in a model prediction Y. Do you observe any multicollinearity? Why or why not? What is different
# 	from the legs example in the chapter?

set.seed(1969 - 08 - 18) # Woodstock seed
n = 1e3

X = runif(n = n, min = 0, max = 100)
Z = rnorm(n = n, mean = 2.5 + 0.4*X, sd = 0.1) # Higly correlated to X
Y = rnorm(n = n, mean = 1.24 + 0.87*Z)

stanData = list(
	N = n,
	X = X,
	Y = Y,
	Z = Z
)

model = cmdstan_model("./temp.stan")
test = model$sample(data = stanData, chains = 4, parallel_chains = 4)

lazyTrace(draws = test$draws("a0"), val1 = 1.24)
lazyTrace(draws = test$draws("a1"), val1 = 0)
lazyTrace(draws = test$draws("a2"), val1 = 0.87)
lazyTrace(draws = test$draws("sigma"), val1 = 1)

# Observations: There is no multicollinearity
# Why: This is a pipe X -> Z -> Y, adding Z to the model just cut the path from X to Y
```

