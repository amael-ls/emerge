---
title: "Simple examples of multivariate models"
date: today
author: AmaÃ«l Le Squin
date-format: iso
filters:
  - fontawesome
  - diagram
execute:
  error: true
bibliography: references.bib
lightbox:
  match: auto
css: style.css
knitr:
  opts_chunk: 
    dev: ragg_png
    crop: null
    out.width: "70%"
    fig.width: 6
    fig.asp: 0.618
    fig.align: "center"
format:
  html:
    toc: true
    include-in-header: mathjax.html
    code-fold: true
    df-print: paged
    number-sections: true
    theme:
      light: cerulean
      dark: darkly
    margin: 5% 0;
  pdf:
    mathspec: true
    include-in-header:
      - text: |
          \usepackage{unicode-math}
---

\newcommand{\ie}{*i.e.,*}
\newcommand{\bOmega}{\symbf{\Omega}}
\newcommand{\bSigma}{\symbf{\Sigma}}

\newcommand{\alphai}{\alpha_{\text{actor}[i], \text{TID}[i]}}
\newcommand{\betai}{\beta_{\text{block}[i], \text{TID}[i]}}
\newcommand{\gammai}{\gamma_{\text{TID}[i]}}

\newcommand{\N}{\mathbfscr{N}}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\mvn}{MVN}
\DeclareMathOperator{\diag}{diag}

## Introduction

The aim of this small file, which might later become an appendix of the main article, is to explore two simple multivariate examples in Stan language. I base my code on two examples developed by @McElreath2020 (chap. 14) about (*i*) a cafe robot and (*ii*) chimpanzees.

### Cafes robot example
Let a robot visit cafes and order a coffee. The robot measures each time how long did it wait, and when it arrived (a simple categorical variable of two values: morning and afternoon). In the chapter 13 of @McElreath2020, it has been shown that the robot learns better when using partial pooling. In the chapter 14, McElreath claims that we can learn even more by also investigating and accounting for the correlation structure. Why? Because popular cafes have a longer waiting time in the morning (rush hour) and improves significantly the afternoon, while there is little difference in waiting time between the morning and the afternoon for non-popular cafes. Therefore, there are two levels of borrowing information:

1. Between the cafes (partial pooling)
2. Between the parameters (covariance)

The @sec-create_data is dedicated to simulate data, which are fit in @#sec-fit by jointly modelling slopes and intercepts. The results are analysed in @sec-results.

### Chimpanzees example
The chimpanzee experiment is fully described in @McElreath2020 (chap. XI). Basically, there is a table with two dishes and a lever on the left and right side, respectively. Pulling a lever moves a dish towards you, and move the other dish towards the other side of the table. On one side, only one dish contains food, which comes to you if you pull the corresponding lever, and on the other side both dishes contains food. The aim is to see if a chimpanzees behave differently when there is a congener at the other end of table or not. Humans, for instance, would pull the lever that provides food to both, which is called the *prosocial* option.

The data are described in @sec-data_chimp, which are fit in @sec-fit_chimp by jointly modelling ???. The results are analysed in @sec-results_chimp.

### Spatial autocorrelation in oceanic tools


## Cafes robot
### Create data {#sec-create_data}

```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

options(max.print = 500)

library(data.table)
library(cmdstanr)
	register_knitr_engine(override = TRUE)
library(stringi)
library(ellipse)
library(MASS)
library(gt)

source("./toolFunctions.R")
```

The parameters are defined as below:
```{r}
#### Population parameters, p. 437
a = 3.5 # Mean intercept (i.e., mean waiting time)
b = -1 # Mean slope (i.e., mean improvement waiting time in the afternoon. Improvement because < 0)

sigma_a = 1 # Variance intercept
sigma_b = 0.5 # Variance change morning/afternoon (improvement if < 0, worsen otherwise)
sigma_cafe = 0.15 # Variance within cafe

rho = -0.7 # Covariance between intercepts and slopes

#### Var-Cov matrix
sigma_mat = diag(c(sigma_a, sigma_b))
rho_mat = matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)

varCov_mat = sigma_mat %*% rho_mat %*% sigma_mat
```

and the data are created using a multivariate normal distribution for the parameters (intercept and slope for each cafe, all coming from the same distribution---group effect and correlation are then included):

```{r}
#### Simulate data
## Simulate parameters
n_cafes = 40
set.seed(5)

params_dt = as.data.table(mvrnorm(n_cafes, c(a, b), varCov_mat))
setnames(params_dt, new = c("alpha_cafe", "beta_cafe"))

plot(params_dt[, alpha_cafe], params_dt[, beta_cafe], pch = 19, xlab = "alpha", ylab = "beta", axes = FALSE)
axis(1)
axis(2, las = 1)
for (l in c(0.1, 0.3, 0.5, 0.8, 0.99))
	lines(ellipse(x = varCov_mat, centre = c(a, b), level = l), col = "#22442233", lwd = 2)

## Simulate waiting times
set.seed(22)
n_visit = 10 # 5 in the morning, 5 in the afternoon

if (n_visit %% 2 != 0)
	stop("n_visit must be an even integer")

visit_dt = data.table(cafe = rep(1:n_cafes, each = n_visit),
	is_afternoon = rep(c(rep(FALSE, n_visit/2), rep(TRUE, n_visit/2)), n_cafes))
setkey(visit_dt, cafe)

for (i in 1:n_cafes)
	visit_dt[.(i), waiting_time := rnorm(n = n_visit,
		mean = params_dt[i, alpha_cafe] + params_dt[i, beta_cafe]*is_afternoon, sd = sigma_cafe)]

visit_dt[, afternoon_int := ifelse(is_afternoon, 1, 0)]
```

This creates an easy, balanced dataset of `{r} n_cafes` cafes visited `{r} n_visit` times (equaly shared between mornings and afternoons).

### Fit the model {#sec-fit}
#### DAG
```{mermaid}
%%| label: fig-model
%%| fig-cap: Diagram Acyclic Graph
graph TB
	Y("$$Y \sim N \left( \mu_{\text{cafe}[i]}, \sigma_{\text{cafe}[i]} \right)$$")
```

#### Stan model
Here is the stan model according to the @fig-model:
```{stan output.var = "model", cache = TRUE, fold = FALSE}
data {
	// Dimensions
	int <lower = 1> N_cafes; // Number of cafes
	int <lower = 1> N_visit; // Number of visits per cafe
	int <lower = N_cafes*N_visit, upper = N_cafes*N_visit> N; // Number of data

	// Explanatory variable
	vector <lower = 0, upper = 1> [N] afternoon; // 0 if morning, 1 otherwise

	// Observations
	vector <lower = 0> [N] wait;
}

parameters {
	// Population parameters...
	// ... intercept and slope
	real a; // Intercept
	real b; // Slope
	
	// ... variances
	real sigma_a;
	real sigma_b;
	cholesky_factor_corr[2] L;

	// Variance (residuals)
	real<lower = 0> sigma; // Variance within same cafe

	// Whatever
	vector [N_cafes] a_cafe;
	vector [N_cafes] b_cafe;
}

transformed parameters {
	vector <lower = 0>[2] sigma_vec = to_vector({sigma_a, sigma_b});
}

model {
	int cc = 0;

	// Population priors
	target += normal_lpdf(a | 5, 2);
	target += normal_lpdf(b | -1, 2);
	
	target +=  lkj_corr_cholesky_lpdf(L | 2); // It contains rho (non-diag)

	// Variance (residuals)
	target += exponential_lpdf(sigma | 1);

	// Likelihood and cafe parameters
	for (i in 1:N_cafes)
	{
		target += multi_normal_cholesky_lpdf(to_vector({a_cafe[i], b_cafe[i]}) | to_vector({a, b}),
			diag_pre_multiply(sigma_vec, L)); // Cafe params
		for (j in 1:N_visit)
		{
			cc = (i - 1)*N_visit + j;
			target += normal_lpdf(wait[cc] | a_cafe[i] + b_cafe[i]*afternoon[cc], sigma); // Likelihood
		}
	}
}

generated quantities {
	cov_matrix[2] Sigma;
	real rho;
	
	// This is to recover the variance-covariance matrix instead of having its Cholesky factor
	Sigma = multiply_lower_tri_self_transpose(diag_pre_multiply(sigma_vec, L));

	// This is to recover the correlation parameter
	rho = tcrossprod(L)[2, 1];
}
```

All the functions are described in the [Stan Functions Reference](https://mc-stan.org/docs/functions-reference/), however, few explanations are required for the multivariate.

Remember that the variance-covariance matrix of a multivariate distribution, $\bSigma$, can be rewritten as a product of 'pure variance' and 'pure covariance' matrices:
$$
\bSigma = \begin{bmatrix} \sigma_a & 0 \\ 0 & \sigma_b \end{bmatrix}
	\begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}
	\begin{bmatrix} \sigma_a & 0 \\ 0 & \sigma_b \end{bmatrix}
$$

The diagonal elements of $\bSigma$ are the variances of the parameters ($\sigma_a^2$ and $\sigma_b^2$), and the non-diagonal elements are the covariances ($\rho \sigma_a \sigma_b$). We denote the diagonal matrix by $\symbf{V}$, and the middle matrix by $\bOmega = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}$. The matrix $\bOmega$ is symmetric positive-definite and can be Cholesky-decomposed:
$$
\bOmega = \symbf{L}\symbf{L}^T,
$$
where the index $T$ is for transpose, and $L$ is a lower triangular matrix. According to [Stan correlation matrix page](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html), it is much better computationally to work directly with the Cholesky factor of $\bSigma$, which is:
\begin{equation}
	\label{eq::cholesky_Sigma}
	\begin{aligned}
		\bSigma &= \symbf{V} \bOmega \symbf{V} \\
			&= \symbf{V} \symbf{L} \symbf{L}^T \symbf{V} \\
			&= (\symbf{V} \symbf{L}) (\symbf{V} \symbf{L})^T.
	\end{aligned}
\end{equation}
We denote by $\symbf{L_{\Sigma}} = \symbf{V} \symbf{L}$ the Cholesky factor of $\bSigma$.

Now that this is explained, the line `target +=  lkj_corr_cholesky_lpdf(L | 2);` is just the prior on $\symbf{L}$. The prior `target +=  lkj_corr_lpdf(\eta)` just put a prior that is skeptical about high correlation for $\eta \geqslant 1$. The higher $\eta$, the more concentrated around the identity matrix is the prior [@McElreath2020]. [Stan correlation matrix page](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html) recommands to work with the Cholesky decomposition, hence the prior on $\symbf{L}$ rather than $\bOmega$ and the use of `lkj_corr_cholesky_lpdf` instead of `lkj_corr_lpdf` (note the _cholesky_ that disappeared in the name).

Then the second step is `diag_pre_multiply(sigma_vec, L)`. This function just transforms a vector (here $(\sigma_a, \sigma_b)$) to a diagonal matrix and then multiply it by $\symbf{L}$, which gives $\symbf{L_{\Sigma}}$ (see equation \eqref{eq::cholesky_Sigma}). Note that this is valid only because I use `multi_normal_cholesky_lpdf` which requires the Cholesky factor of $\bSigma$, $\symbf{L_{\Sigma}}$, contrary to `multi_normal_lpdf`.

#### Stan data
```{r}
#### Fit the model
## Stan data
stanData = list(
	N_cafes = n_cafes, # Number of cafes
	N_visit = n_visit, # Number of visits per cafe
	N = n_cafes*n_visit,
	afternoon = visit_dt[, afternoon_int],
	wait = visit_dt[, waiting_time]
)

## Common variables
n_chains = 4

iter_warmup = 1000
iter_sampling = 1000
```

#### Run the model
```{r}
#| output: false
#| fold: false
## Run
fit = model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)
```

### Results {#sec-results}
The posterior of $\rho$ (which equals to `{r} rho`) can be extracted by multiplying $\symbf{L}$ by its transpose, and then take the subdiagonal value $\symbf{L}\symbf{L}^T[2, 1]$:
```{r}
rho_posterior = fit$draws("rho", inc_warmup = FALSE)
aa = lazyPosterior(rho_posterior, val1 = rho)
```

The posterior of the population parameters are:
```{r}
#| echo: false
#| label: fig-posteriorPop
#| fig-cap: "Posterior of the population parameters"
#| fig-subcap: 
#|   - "$a$"
#|   - "$b$"
#| layout-ncol: 2
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("a", inc_warmup = FALSE), val1 = a)
aa = lazyPosterior(fit$draws("b", inc_warmup = FALSE), val1 = b)
```

The posterior of the variances are:
```{r}
#| echo: false
#| label: fig-posteriorVar
#| fig-cap: "Posterior of the variances"
#| fig-subcap: 
#|   - "$\\sigma_a$"
#|   - "$\\sigma_b$"
#|   - "$\\sigma$"
#| layout-ncol: 3
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("sigma_a", inc_warmup = FALSE), val1 = sigma_a)
aa = lazyPosterior(fit$draws("sigma_b", inc_warmup = FALSE), val1 = sigma_b)
aa = lazyPosterior(fit$draws("sigma", inc_warmup = FALSE), val1 = sigma_cafe)
```

And the posterior of the covariance (which is $\rho \sigma_a \sigma_b$)
```{r}
#| echo: false
#| label: fig-posteriorCov
#| fig-cap: "Posterior of the covariance"
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("Sigma")[, , "Sigma[2,1]"], val1 = sigma_a*sigma_b*rho)
```

## Chimpanzee
### Data description {#sec-data_chimp}
```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

library(rethinking)

source("./toolFunctions.R")

data(chimpanzees)
setDT(chimpanzees)

n_chimp = length(unique(chimpanzees[, actor]))
n_block = length(unique(chimpanzees[, block]))
```

There are eight variables in the `chimpanzees` dataset:

1. actor: name of the target chimpanzee (which pull the levers)
2. recipient: name of recipient (NA for partner absent condition)
3. condition: partner absent (0), partner present (1)
4. block: block of trials (each actor x each recipient 1 time)
5. trial: trial number (by chimp = ordinal sequence of trials for each chimp, ranges from 1-72; partner present trials were interspersed with partner absent trials)
6. prosocial_left: 1 if prosocial (1/1) option was on left
7. chose_prosoc: choice chimp made (0 = 1/0 option, 1 = 1/1 option)
8. pulled_left: which side did chimp pull (1 = left, 0 = right)

This dataset is **cross-classified** because actors are not nested within blocks. The `block` represents the batch of experiment, I think this variable could have been called `day` or `batch_id`. Within a day, all the chimpanzees will be actor $2(n - 1)$ times: $n - 1$ times alone, and $n - 1$ times with a partner, where $n$ is the number of chimpanzees. For the recipient, I think they do a $+1$ to the id of the chimpanzees: a chimpanzee $i$ will be in relation with all the other chimpanzee but $i + 1$ (which is I guess himself...). That's a bit odd...

In total, there are `{r} n_chimp` chimpanzees and `{r} n_block` blocks (or batches of experiment).

### Fit the model {#sec-fit_chimp}
We are interested in predicting `pulled_left`, $L_i$, which indicates if the studied chimpanzee pulled the left lever (value 1) or the right lever (value 0). The predictors are `prosoc_left`, which indicates if the *prosocial* choice was associated to the left lever, and `condition`, which indicates the congener presence. That creates four treatments:

1. R/N: Right lever is the prosocial choice, no partner
2. L/N: Left lever is the prosocial choice, no partner
3. R/P: Right lever is the prosocial choice, presence of a partner
4. L/P: Left lever is the prosocial choice, presence of a partner

```{r}
treatment_fct = function(prosoc_lever, partner)
{
	results = rep(-Inf, length = length(prosoc_lever))

	# Right lever is prosocial and...
	results[which(prosoc_lever == 0 & partner == 0)] = 1 # ...no partner
	results[which(prosoc_lever == 0 & partner == 1)] = 3 # ...with partner
	
	# Left lever is prosocial and...
	results[which(prosoc_lever == 1 & partner == 0)] = 2 # ...no partner
	results[which(prosoc_lever == 1 & partner == 1)] = 4 # ...with partner

	return(results)
}

chimpanzees[, treatment := treatment_fct(prosoc_left, condition)]
setkey(x = chimpanzees, actor, block, treatment)
```

#### Mathematical formulation
\begin{equation}
	\begin{aligned}
		L_i &\sim \text{Binomial}(1, p_i) \\
		\logit(p_i) &= \gammai + \alphai + \betai,
	\end{aligned}
\end{equation}
where $\gammai$ is the average odds (more precisely log-odds because on log scale) for each treatment, which is then modified by each actor in each treatment, $\alphai$, and by each block in each treatment, $\betai$.

More specifically, it is possible to rewrite this model with dummy variables, like what would be done in `lme4`. Here is an idea of how it would work in absence of correlation between parameters for the second treatment (hence a 1 at the second position of the vector), chimpanzee 1 (hence the first batch of vector containing the 1, the other values all being 0):

\begin{align*}
	\logit(p_i) &=
		\begin{bmatrix} \gamma_1 & & \\ & \ddots & \\ & & \gamma_4 \end{bmatrix}
		\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} +

		\begin{bmatrix}
			\begin{bmatrix}
				\alpha_{1, 1} & & \\ & \ddots & \\ & & \alpha_{1, 4}
			\end{bmatrix} & & \\
			& \ddots & \\
			& & \begin{bmatrix}
				\alpha_{j, 1} & & \\ & \ddots & \\ & & \alpha_{j, 4}
			\end{bmatrix}
		\end{bmatrix}
		\begin{pmatrix}
			\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} \\
			\vdots \\
			\begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}
		\end{pmatrix} + \\

		& \qquad \begin{bmatrix}
			\begin{bmatrix}
				\beta_{1, 1} & & \\ & \ddots & \\ & & \beta_{1, 4}
			\end{bmatrix} & & \\
			& \ddots & \\
			& & \begin{bmatrix}
				\beta_{j, 1} & & \\ & \ddots & \\ & & \beta_{j, 4}
			\end{bmatrix}
		\end{bmatrix}
		\begin{pmatrix}
			\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} \\
			\vdots \\
			\begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}
		\end{pmatrix} \\
	&= \symbf{\Gamma} \symbf{v_1} + \symbf{A} \symbf{v_2} + \symbf{B} \symbf{v_3}
\end{align*}

The matrix $\symbf{A}$ has a diagonal of `{r} n_chimp*4` elements ($4 \times \text{number chimp}$), and the matrix $\symbf{B}$ has a diagonal of `{r} n_block*4` elements ($4 \times \text{number blocks}$). The unit vectors $v_i, \, 1 \geqslant i \geqslant 3$ have a single non-zero value.

Now, we consider a model with correlations between parameters belonging to a same cluster (here, two clusters: actors and blocks). The vector $v_2$ of `{r} n_block*4` elements becomes an array of `{r} n_chimp` vectors of length 4. Each vector is independent of the others and is drawn from the same multivariate normal distribution, which accounts for the correlation between the elements within each vector. Similarly the vector $v_3$ becomes an array of `{r} n_block` vectors of length 4. These model's details are in the Stan code (@sec-stan_chimp).

#### Stan model, centred parametrisation {#sec-stan_chimp}
```{stan output.var = "model_chimp", cache = TRUE, fold = FALSE}
data {
	// Dimensions
	int <lower = 1> N; // Number of data
	int <lower = 1> N_chimp; // Number of chimpanzees
	int <lower = 1> N_block; // Number of blocks
	int <lower = 1> N_treatment; // Number of treatments
	int <lower = 2*(N_chimp - 1), upper = 2*(N_chimp - 1)> N_measure; // Number of measure per group (actor, block)

	// Explanatory variable
	array[N] int <lower = 1, upper = N_treatment> treatment; // Which treatment is applied for observation i
	vector <lower = 0, upper = 1> [N] left_prosocial; // Boolean 1 if left lever is prosocial, 0 otherwise
	vector <lower = 0, upper = N_block> [N] block_id; // Block id, from 1 to N_block

	// Observations
	array[N] int <lower = 0, upper = 1> left_pull; // Boolean, 1 if left lever pulled, 0 otherwise
}

parameters {
	// Population parameters...
	// ... intercept and slope
	vector[N_treatment] gamma; // Intercept
	array[N_chimp] vector[N_treatment] alpha; // Slope actor
	array[N_block] vector[N_treatment] beta_; // Slope block
	
	// Variances (in the variance-covariance matrices)
	vector<lower = 0> [N_treatment] sigma_diag_actor;
	vector<lower = 0> [N_treatment] sigma_diag_block;
	cholesky_factor_corr[N_treatment] L_actor;
	cholesky_factor_corr[N_treatment] L_block;
}

model {
	real odds = 0;
	int count = 1;
	// Priors...
	// ... variance
	target += lkj_corr_cholesky_lpdf(L_actor | 2);
	target += lkj_corr_cholesky_lpdf(L_block | 2);
	target += exponential_lpdf(sigma_diag_actor | 1);
	target += exponential_lpdf(sigma_diag_block | 1);

	// ... slopes
	target += multi_normal_cholesky_lpdf(alpha | rep_vector(0, N_treatment),
		diag_pre_multiply(sigma_diag_actor, L_actor)); // Vectorised, applied to each vector of the array
	target += multi_normal_cholesky_lpdf(beta_ | rep_vector(0, N_treatment),
		diag_pre_multiply(sigma_diag_block, L_block)); // Vectorised, applied to each vector of the array

	// ... Intercept
	target += normal_lpdf(gamma | 0, 1); // Vectorised, applied to each element of the vector

	// Likelihood and cafe parameters
	for (ch in 1:N_chimp)
	{
		for (bl in 1:N_block)
		{
			for (i in 1:N_measure)
			{
				odds = gamma[treatment[count]] + alpha[ch][treatment[count]] + beta_[bl][treatment[count]];
				target += bernoulli_logit_lpmf(left_pull[count] | odds);
				count = count + 1;
			}
		}
	}
}
```

#### Stan model, non-centred parametrisation {#sec-stan_chimp_nc}
We will see it later, but the centred parametrisation creates divergences, probably due to a funnel-like problem. For the non-centred parametrisation, it is important to remember the following property:

:::{.callout-tip}

## Centred to non-centred parametrisation

Let $X \sim \mvn(\mu, \bSigma)$, then it is equivalent to:

\begin{align*}
	X &= \mu + \symbf{A}Z \\
	Z &\sim \mvn(0, \symbf{\mathds{1}}) \\
	\bSigma &= \symbf{A}\symbf{A}^T,
\end{align*}

where $Z$ is a multivariate normally distributed vector, $\mathds{1}}$ is the identity matrix, and $\symbf{A}$ a matrix. Note that it is then possible to use $\symbf{A} = \diag(\sigma) \symbf{L}$, with $\symbf{L}$ the Cholesky factor.

:::

```{stan output.var = "model_chimp_nc", cache = TRUE, fold = FALSE}
/*
Remarks:
	1. Note that there is no residual variance here! This is because for Bernoulli variables, B(p), the odds are p,
		and the variance is p(1 - p). So only one parameter!
	2. The non-centred parametrisation
*/

data {
	// Dimensions
	int <lower = 1> N; // Number of data
	int <lower = 1> N_chimp; // Number of chimpanzees
	int <lower = 1> N_block; // Number of blocks
	int <lower = 1> N_treatment; // Number of treatments
	int <lower = 2*(N_chimp - 1), upper = 2*(N_chimp - 1)> N_measure; // Number of measure per group (actor, block)

	// Explanatory variable
	array[N] int <lower = 1, upper = N_treatment> treatment; // Which treatment is applied for observation i
	vector <lower = 0, upper = 1> [N] left_prosocial; // Boolean 1 if left lever is prosocial, 0 otherwise
	vector <lower = 0, upper = N_block> [N] block_id; // Block id, from 1 to N_block

	// Observations
	array[N] int <lower = 0, upper = 1> left_pull; // Boolean, 1 if left lever pulled, 0 otherwise
}

parameters {
	// Population parameters...
	// ... intercept and slopes
	vector[N_treatment] gamma; // Intercept
	array[N_chimp] vector[N_treatment] Z_chimp; // Actor effect
	array[N_block] vector[N_treatment] Z_block; // Block effect
	
	// Variances (in the variance-covariance matrices)
	vector<lower = 0> [N_treatment] sigma_diag_actor;
	vector<lower = 0> [N_treatment] sigma_diag_block;
	cholesky_factor_corr[N_treatment] L_actor;
	cholesky_factor_corr[N_treatment] L_block;
}

transformed parameters {
	array[N_chimp] vector[N_treatment] alpha; // Slope actor
	array[N_block] vector[N_treatment] beta_; // Slope block

	for (ch in 1:N_chimp)
		alpha[ch] = diag_pre_multiply(sigma_diag_actor, L_actor) * Z_chimp[ch];
	
	for (bl in 1:N_block)
		beta_[bl] = diag_pre_multiply(sigma_diag_block, L_block) * Z_block[bl];
}

model {
	real odds = 0;
	int count = 1;
	// Priors...
	// ... variance
	target += lkj_corr_cholesky_lpdf(L_actor | 2);
	target += lkj_corr_cholesky_lpdf(L_block | 2);
	target += exponential_lpdf(sigma_diag_actor | 1);
	target += exponential_lpdf(sigma_diag_block | 1);

	// ... slopes
	target += multi_normal_lpdf(Z_chimp | rep_vector(0, N_treatment), identity_matrix(N_treatment));
	target += multi_normal_lpdf(Z_block | rep_vector(0, N_treatment), identity_matrix(N_treatment));

	// ... Intercept
	target += normal_lpdf(gamma | 0, 1); // Vectorised, applied to each element of the vector

	// Likelihood and cafe parameters
	for (ch in 1:N_chimp)
	{
		for (bl in 1:N_block)
		{
			for (i in 1:N_measure)
			{
				odds = gamma[treatment[count]] + alpha[ch][treatment[count]] + beta_[bl][treatment[count]];
				target += bernoulli_logit_lpmf(left_pull[count] | odds);
				count = count + 1;
			}
		}
	}
}

generated quantities {
	cov_matrix[N_treatment] Sigma_actor;
	cov_matrix[N_treatment] Sigma_block;
	vector[N] new_sim;
	
	// This is to recover the variance-covariance matrix instead of having its Cholesky factor
	Sigma_actor = multiply_lower_tri_self_transpose(diag_pre_multiply(sigma_diag_actor, L_actor));
	Sigma_block = multiply_lower_tri_self_transpose(diag_pre_multiply(sigma_diag_block, L_block));

	{
		int count = 1;
		real odds = 0;

		for (ch in 1:N_chimp)
		{
			for (bl in 1:N_block)
			{
				for (i in 1:N_measure)
				{
					odds = gamma[treatment[count]] + alpha[ch][treatment[count]] + beta_[bl][treatment[count]];
					new_sim[count] = bernoulli_logit_rng(odds);
					count = count + 1;
				}
			}
		}
	}
}
```

#### Stan data
```{r}
## Prepare data
stanData = list(
	N = chimpanzees[, .N],
	N_chimp = n_chimp,
	N_block = n_block,
	N_treatment = 4,
	N_measure = 2*(n_chimp - 1),

	treatment = chimpanzees[, treatment],
	left_prosocial = chimpanzees[, prosoc_left],
	block_id = chimpanzees[, block],

	left_pull = chimpanzees[, pulled_left]
)

model = cmdstan_model("./04_test.stan")
model_bis = cmdstan_model("./04_test_bis.stan")

## Common variables
n_chains = 4

iter_warmup = 1000
iter_sampling = 1000
```

#### Run the model
```{r}
#| output: false
#| fold: false
## Run
fit = model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)

fit_nc = model_bis$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)
```

### Results {#sec-results_chimp}

```{r}
gamma_est = round(apply(X = fit_nc$draws("gamma", inc_warmup = FALSE), MARGIN = 3, FUN = mean), 2)
sigma_act_est = round(apply(X = fit_nc$draws("sigma_diag_actor", inc_warmup = FALSE), MARGIN = 3, FUN = mean), 2)
sigma_blo_est = round(apply(X = fit_nc$draws("sigma_diag_block", inc_warmup = FALSE), MARGIN = 3, FUN = mean), 2)
```

```{r}
lazyPosterior(fit_nc$draws("gamma[1]", inc_warmup = FALSE), fun = dnorm, mean = 0, sd = 1,
	xlab = "gamma[1]")
lazyPosterior(fit_nc$draws("sigma_diag_actor[1]", inc_warmup = FALSE), fun = dexp, rate = 1,
	xlab = "sigma_diag_actor[1]")
```

Before finishing this example, let us check the average posterior predictions for the block 5, as done in @McElreath2020 (p. 451-452). For this, I use the generated quantity `new_sim`:

```{r}

ind_bl_5 = chimpanzees[, which(block == 5)]
n_measure = 2*(n_chimp - 1)

dt = data.table(chimp = rep(1:n_chimp, each = 4), treatment = rep(1:4, n_chimp), avg = rep(-Inf, 4*n_chimp),
	pch = rep(c(1, 1, 19, 19), n_chimp), key = c("chimp", "treatment"))

posterior_sim = fit_nc$draws("new_sim", inc_warmup = FALSE)[, , ind_bl_5]

for (i in seq_len(n_chimp))
{
	for (tt in 1:4)
	{
		ind = chimpanzees[ind_bl_5][.(i), which(treatment == tt)] + (i - 1)*n_measure
		dt[.(i, tt), avg := mean(posterior_sim[, , ind])]
	}
}

plot(1:dt[, .N], dt[, avg], axes = FALSE, xlab = "", ylab = "Prop. left lever", ylim = c(0, 1),
	pch = dt[, pch])
abline(h = 0.5, col = "#11223322")
for (i in 2:n_chimp - 1)
	abline(v = 4*i + 0.5, lty = "dashed", col = "#11223322")
segments(x0 = seq(1, dt[, .N], by = 4), y0 = dt[seq(1, .N, by = 4), avg],
	x1 = seq(3, dt[, .N], by = 4), y1 = dt[seq(3, .N, by = 4), avg])
segments(x0 = seq(2, dt[, .N], by = 4), y0 = dt[seq(2, .N, by = 4), avg],
	x1 = seq(4, dt[, .N], by = 4), y1 = dt[seq(4, .N, by = 4), avg])
axis(1)
axis(2, las = 1, at = c(0, 0.5, 1))
```

This ends the Chimpanzee example. There is a third example about dyad in households (pairs of households that exchange gifts), but I will not do it now, as it does not bring much more stuff.

However, the next example in @McElreath2020 (p. 468) talks about continuous categories and the Gaussian process. So far, the examples only had discrete variables such as cafes, block, chimpanzee. But what about continuous variables such as age or income? People of the same age for instance experienced many things in common (music, political events, ...). People of *similar* age (\ie close but not equal) were also exposed to similar stuff (hence the term generation, which span over few years) although to a lesser degree than people of the same age. It would not make sense to estimate a unique intercept for individuals of the same age and ignoring the fact that people of similar age should have more similar intercepts. The general approach is known as **Gaussian process regression** and is the topic of the next example

## Spatial autocorrelation in Oceanic tools

### Data description {#sec-data_ocean}

### Fit the model {#sec-fit_ocean}

Short description


#### Mathematical formulation

Maths eq


#### Stan model, non-centred parametrisation {#sec-stan_ocean_nc}
```{stan output.var = "model_ocean", cache = TRUE, fold = FALSE}
data {
	// Dimensions


	// Explanatory variable

	// Observations

}

parameters {

}

transformed parameters {

}

model {

}

generated quantities {
}
```


#### Stan data
```{r}
## Prepare data
stanData = list(

)

## Common variables
n_chains = 4

iter_warmup = 1000
iter_sampling = 1000
```

#### Run the model
```{r}
#| output: false
#| fold: false
## Run
fit = model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)

fit_nc = model_bis$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)
```

### Results {#sec-results_ocean}