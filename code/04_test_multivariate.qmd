---
title: "Simple examples of multivariate models"
date: today
author: AmaÃ«l Le Squin
date-format: iso
filters:
  - fontawesome
  - diagram
execute:
  error: true
bibliography: references.bib
lightbox:
  match: auto
css: style.css
knitr:
  opts_chunk: 
    dev: ragg_png
    crop: null
    out.width: "70%"
    fig.width: 6
    fig.asp: 0.618
    fig.align: "center"
format:
  html:
    toc: true
    include-in-header: mathjax.html
    code-fold: true
    df-print: paged
    number-sections: true
    theme:
      light: cerulean
      dark: darkly
    margin: 5% 0;
  pdf:
    mathspec: true
    include-in-header:
      - text: |
          \usepackage{unicode-math}
---

\newcommand{\ie}{*i.e.,*}
\newcommand{\bOmega}{\symbf{\Omega}}
\newcommand{\bSigma}{\symbf{\Sigma}}

\newcommand{\alphai}{\alpha_{\text{actor}[i], \text{TID}[i]}}
\newcommand{\betai}{\beta_{\text{block}[i], \text{TID}[i]}}
\newcommand{\gammai}{\gamma_{\text{TID}[i]}}

\newcommand{\N}{\mathbfscr{N}}
\DeclareMathOperator{\logit}{logit}

## Introduction

The aim of this small file, which might later become an appendix of the main article, is to explore two simple multivariate examples in Stan language. I base my code on two examples developed by @McElreath2020 (chap. 14) about (*i*) a cafe robot and (*ii*) chimpanzees.

### Cafes robot example
Let a robot visit cafes and order a coffee. The robot measures each time how long did it wait, and when it arrived (a simple categorical variable of two values: morning and afternoon). In the chapter 13 of @McElreath2020, it has been shown that the robot learns better when using partial pooling. In the chapter 14, McElreath claims that we can learn even more by also investigating and accounting for the correlation structure. Why? Because popular cafes have a longer waiting time in the morning (rush hour) and improves significantly the afternoon, while there is little difference in waiting time between the morning and the afternoon for non-popular cafes. Therefore, there are two levels of borrowing information:

1. Between the cafes (partial pooling)
2. Between the parameters (covariance)

The @sec-create_data is dedicated to simulate data, which are fit in @#sec-fit by jointly modelling slopes and intercepts. The results are analysed in @sec-results.

### Chimpanzees example
The chimpanzee experiment is fully described in @McElreath2020 (chap. XI). Basically, there is a table with two dishes and a lever on the left and right side, respectively. Pulling a lever moves a dish towards you, and move the other dish towards the other side of the table. On one side, only one dish contains food, which comes to you if you pull the corresponding lever, and on the other side both dishes contains food. The aim is to see if a chimpanzees behave differently when there is a congener at the other end of table or not. Humans, for instance, would pull the lever that provides food to both, which is called the *prosocial* option.

The data are described in @sec-data_chimp, which are fit in @sec-fit_chimp by jointly modelling ???. The results are analysed in @sec-results_chimp.

## Cafes robot
### Create data {#sec-create_data}

```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

options(max.print = 500)

library(data.table)
library(cmdstanr)
	register_knitr_engine(override = TRUE)
library(stringi)
library(ellipse)
library(MASS)
library(gt)

source("./toolFunctions.R")
```

The parameters are defined as below:
```{r}
#### Population parameters, p. 437
a = 3.5 # Mean intercept (i.e., mean waiting time)
b = -1 # Mean slope (i.e., mean improvement waiting time in the afternoon. Improvement because < 0)

sigma_a = 1 # Variance intercept
sigma_b = 0.5 # Variance change morning/afternoon (improvement if < 0, worsen otherwise)
sigma_cafe = 0.15 # Variance within cafe

rho = -0.7 # Covariance between intercepts and slopes

#### Var-Cov matrix
sigma_mat = diag(c(sigma_a, sigma_b))
rho_mat = matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)

varCov_mat = sigma_mat %*% rho_mat %*% sigma_mat
```

and the data are created using a multivariate normal distribution for the parameters (intercept and slope for each cafe, all coming from the same distribution---group effect and correlation are then included):

```{r}
#### Simulate data
## Simulate parameters
n_cafes = 40
set.seed(5)

params_dt = as.data.table(mvrnorm(n_cafes, c(a, b), varCov_mat))
setnames(params_dt, new = c("alpha_cafe", "beta_cafe"))

plot(params_dt[, alpha_cafe], params_dt[, beta_cafe], pch = 19, xlab = "alpha", ylab = "beta", axes = FALSE)
axis(1)
axis(2, las = 1)
for (l in c(0.1, 0.3, 0.5, 0.8, 0.99))
	lines(ellipse(x = varCov_mat, centre = c(a, b), level = l), col = "#22442233", lwd = 2)

## Simulate waiting times
set.seed(22)
n_visit = 10 # 5 in the morning, 5 in the afternoon

if (n_visit %% 2 != 0)
	stop("n_visit must be an even integer")

visit_dt = data.table(cafe = rep(1:n_cafes, each = n_visit),
	is_afternoon = rep(c(rep(FALSE, n_visit/2), rep(TRUE, n_visit/2)), n_cafes))
setkey(visit_dt, cafe)

for (i in 1:n_cafes)
	visit_dt[.(i), waiting_time := rnorm(n = n_visit,
		mean = params_dt[i, alpha_cafe] + params_dt[i, beta_cafe]*is_afternoon, sd = sigma_cafe)]

visit_dt[, afternoon_int := ifelse(is_afternoon, 1, 0)]
```

This creates an easy, balanced dataset of `{r} n_cafes` cafes visited `{r} n_visit` times (equaly shared between mornings and afternoons).

### Fit the model {#sec-fit}
#### DAG
```{mermaid}
%%| label: fig-model
%%| fig-cap: Diagram Acyclic Graph
graph TB
	Y("$$Y \sim N \left( \mu_{\text{cafe}[i]}, \sigma_{\text{cafe}[i]} \right)$$")
```

#### Stan model
Here is the stan model according to the @fig-model:
```{stan output.var = "model", cache = TRUE, fold = FALSE}
data {
	// Dimensions
	int <lower = 1> N_cafes; // Number of cafes
	int <lower = 1> N_visit; // Number of visits per cafe
	int <lower = N_cafes*N_visit, upper = N_cafes*N_visit> N; // Number of data

	// Explanatory variable
	vector <lower = 0, upper = 1> [N] afternoon; // 0 if morning, 1 otherwise

	// Observations
	vector <lower = 0> [N] wait;
}

parameters {
	// Population parameters...
	// ... intercept and slope
	real a; // Intercept
	real b; // Slope
	
	// ... variances
	real sigma_a;
	real sigma_b;
	cholesky_factor_corr[2] L;

	// Variance (residuals)
	real<lower = 0> sigma; // Variance within same cafe

	// Whatever
	vector [N_cafes] a_cafe;
	vector [N_cafes] b_cafe;
}

transformed parameters {
	vector <lower = 0>[2] sigma_vec = to_vector({sigma_a, sigma_b});
}

model {
	int cc = 0;

	// Population priors
	target += normal_lpdf(a | 5, 2);
	target += normal_lpdf(b | -1, 2);
	
	target +=  lkj_corr_cholesky_lpdf(L | 2); // It contains rho (non-diag)

	// Variance (residuals)
	target += exponential_lpdf(sigma | 1);

	// Likelihood and cafe parameters
	for (i in 1:N_cafes)
	{
		target += multi_normal_cholesky_lpdf(to_vector({a_cafe[i], b_cafe[i]}) | to_vector({a, b}),
			diag_pre_multiply(sigma_vec, L)); // Cafe params
		for (j in 1:N_visit)
		{
			cc = (i - 1)*N_visit + j;
			target += normal_lpdf(wait[cc] | a_cafe[i] + b_cafe[i]*afternoon[cc], sigma); // Likelihood
		}
	}
}

generated quantities {
	cov_matrix[2] Sigma;
	real rho;
	
	// This is to recover the variance-covariance matrix instead of having its Cholesky factor
	Sigma = multiply_lower_tri_self_transpose(diag_pre_multiply(sigma_vec, L));

	// This is to recover the correlation parameter
	rho = tcrossprod(L)[2, 1];
}
```

All the functions are described in the [Stan Functions Reference](https://mc-stan.org/docs/functions-reference/), however, few explanations are required for the multivariate.

Remember that the variance-covariance matrix of a multivariate distribution, $\bSigma$, can be rewritten as a product of 'pure variance' and 'pure covariance' matrices:
$$
\bSigma = \begin{bmatrix} \sigma_a & 0 \\ 0 & \sigma_b \end{bmatrix}
	\begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}
	\begin{bmatrix} \sigma_a & 0 \\ 0 & \sigma_b \end{bmatrix}
$$

The diagonal elements of $\bSigma$ are the variances of the parameters ($\sigma_a^2$ and $\sigma_b^2$), and the non-diagonal elements are the covariances ($\rho \sigma_a \sigma_b$). We denote the diagonal matrix by $\symbf{V}$, and the middle matrix by $\bOmega = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}$. The matrix $\bOmega$ is symmetric positive-definite and can be Cholesky-decomposed:
$$
\bOmega = \symbf{L}\symbf{L}^T,
$$
where the index $T$ is for transpose, and $L$ is a lower triangular matrix. According to [Stan correlation matrix page](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html), it is much better computationally to work directly with the Cholesky factor of $\bSigma$, which is:
\begin{equation}
	\label{eq::cholesky_Sigma}
	\begin{aligned}
		\bSigma &= \symbf{V} \bOmega \symbf{V} \\
			&= \symbf{V} \symbf{L} \symbf{L}^T \symbf{V} \\
			&= (\symbf{V} \symbf{L}) (\symbf{V} \symbf{L})^T.
	\end{aligned}
\end{equation}
We denote by $\symbf{L_{\Sigma}} = \symbf{V} \symbf{L}$ the Cholesky factor of $\bSigma$.

Now that this is explained, the line `target +=  lkj_corr_cholesky_lpdf(L | 2);` is just the prior on $\symbf{L}$. The prior `target +=  lkj_corr_lpdf(\eta)` just put a prior that is skeptical about high correlation for $\eta \geqslant 1$. The higher $\eta$, the more concentrated around the identity matrix is the prior [@McElreath2020]. [Stan correlation matrix page](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html) recommands to work with the Cholesky decomposition, hence the prior on $\symbf{L}$ rather than $\bOmega$ and the use of `lkj_corr_cholesky_lpdf` instead of `lkj_corr_lpdf` (note the _cholesky_ that disappeared in the name).

Then the second step is `diag_pre_multiply(sigma_vec, L)`. This function just transforms a vector (here $(\sigma_a, \sigma_b)$) to a diagonal matrix and then multiply it by $\symbf{L}$, which gives $\symbf{L_{\Sigma}}$ (see equation \eqref{eq::cholesky_Sigma}). Note that this is valid only because I use `multi_normal_cholesky_lpdf` which requires the Cholesky factor of $\bSigma$, $\symbf{L_{\Sigma}}$, contrary to `multi_normal_lpdf`.

#### Stan data
```{r}
#### Fit the model
## Stan data
stanData = list(
	N_cafes = n_cafes, # Number of cafes
	N_visit = n_visit, # Number of visits per cafe
	N = n_cafes*n_visit,
	afternoon = visit_dt[, afternoon_int],
	wait = visit_dt[, waiting_time]
)

## Common variables
n_chains = 4

iter_warmup = 1000
iter_sampling = 1000
```

#### Run the model
```{r}
#| output: false
#| fold: false
## Run
fit = model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)
```

### Results {#sec-results}
The posterior of $\rho$ (which equals to `{r} rho`) can be extracted by multiplying $\symbf{L}$ by its transpose, and then take the subdiagonal value $\symbf{L}\symbf{L}^T[2, 1]$:
```{r}
rho_posterior = fit$draws("rho", inc_warmup = FALSE)
aa = lazyPosterior(rho_posterior, val1 = rho)
```

The posterior of the population parameters are:
```{r}
#| echo: false
#| label: fig-posteriorPop
#| fig-cap: "Posterior of the population parameters"
#| fig-subcap: 
#|   - "$a$"
#|   - "$b$"
#| layout-ncol: 2
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("a", inc_warmup = FALSE), val1 = a)
aa = lazyPosterior(fit$draws("b", inc_warmup = FALSE), val1 = b)
```

The posterior of the variances are:
```{r}
#| echo: false
#| label: fig-posteriorVar
#| fig-cap: "Posterior of the variances"
#| fig-subcap: 
#|   - "$\\sigma_a$"
#|   - "$\\sigma_b$"
#|   - "$\\sigma$"
#| layout-ncol: 3
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("sigma_a", inc_warmup = FALSE), val1 = sigma_a)
aa = lazyPosterior(fit$draws("sigma_b", inc_warmup = FALSE), val1 = sigma_b)
aa = lazyPosterior(fit$draws("sigma", inc_warmup = FALSE), val1 = sigma_cafe)
```

And the posterior of the covariance (which is $\rho \sigma_a \sigma_b$)
```{r}
#| echo: false
#| label: fig-posteriorCov
#| fig-cap: "Posterior of the covariance"
#| fig-asp: 1
#| out-width: 95%
#| fig-width: 8.142857

aa = lazyPosterior(fit$draws("Sigma")[, , "Sigma[2,1]"], val1 = sigma_a*sigma_b*rho)
```

## Chimpanzee
### Data description {#sec-data_chimp}
```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

library(rethinking)

source("./toolFunctions.R")

data(chimpanzees)
setDT(chimpanzees)
```

There are eight variables in the `chimpanzees` dataset:

1. actor: name of the target chimpanzee (which pull the levers)
2. recipient: name of recipient (NA for partner absent condition)
3. condition: partner absent (0), partner present (1)
4. block: block of trials (each actor x each recipient 1 time)
5. trial: trial number (by chimp = ordinal sequence of trials for each chimp, ranges from 1-72; partner present trials were interspersed with partner absent trials)
6. prosocial_left: 1 if prosocial (1/1) option was on left
7. chose_prosoc: choice chimp made (0 = 1/0 option, 1 = 1/1 option)
8. pulled_left: which side did chimp pull (1 = left, 0 = right)

This dataset is **cross-classified** because actors are not nested within blocks. The `block` represents the batch of experiment, I think this variable could have been called `day` or `batch_id`. Within a day, all the chimpanzees will be actor $2(n - 1)$ times: $n - 1$ times alone, and $n - 1$ times with a partner, where $n$ is the number of chimpanzees. For the recipient, I think they do a $+1$ to the id of the chimpanzees: a chimpanzee $i$ will be in relation with all the other chimpanzee but $i + 1$ (which is I guess himself...). That's a bit odd...

In total, there are `{r} length(unique(chimpanzees[, actor]))` chimpanzees and `length(unique(chimpanzees[, block]))` blocks (or batches of experiment).

### Fit the model {#sec-fit_chimp}
We are interested in predicting `pulled_left`, $L_i$, which indicates if the studied chimpanzee pulled the left lever (value 1) or the right lever (value 0). The predictors are `prosoc_left`, which indicates if the *prosocial* choice was associated to the left lever, and `condition`, which indicates the congener presence. That creates four treatments:

1. R/N: Right lever is the prosocial choice, no partner
2. L/N: Left lever is the prosocial choice, no partner
3. R/P: Right lever is the prosocial choice, presence of a partner
4. L/P: Left lever is the prosocial choice, presence of a partner

#### Mathematical formulation
\begin{equation}
	\begin{aligned}
		L_i &\sim \text{Binomial}(1, p_i) \\
		\logit(p_i) &= \gammai + \alphai + \betai,
	\end{aligned}
\end{equation}
where $\gammai$ is the average odds (more precisely log-odds because on log scale) for each treatment, which is then modified by each actor in each treatment, $\alphai$, and by each block in each treatment, $\betai$.

#### Stan model

#### Stan data
```{r}
n_chimp = length(unique(chimpanzees[, actor]))
n_block = length(unique(chimpanzees[, block]))

stanData = list(
	n_chimp = n_chimp,
	n_block = n_block
)
```


#### Run the model

### Results {#sec-results_chimp}