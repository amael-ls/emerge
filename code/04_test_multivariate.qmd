---
title: "A simple example of a multivariate model"
date: today
author: AmaÃ«l Le Squin
date-format: iso
filters:
  - fontawesome
  - diagram
execute:
  error: true
bibliography: references.bib
lightbox:
  match: auto
css: style.css
knitr:
  opts_chunk: 
    dev: ragg_png
    crop: null
    out.width: "70%"
    fig.width: 6
    fig.asp: 0.618
    fig.align: "center"
format:
  html:
    toc: true
    include-in-header: mathjax.html
    code-fold: true
    df-print: paged
    number-sections: true
    theme:
      light: cerulean
      dark: darkly
    margin: 5% 0;
  pdf:
    mathspec: true
    include-in-header:
      - text: |
          \usepackage{unicode-math}
---

\newcommand{\ie}{*i.e.,*}
\newcommand{\bOmega}{\symbf{\Omega}}
\newcommand{\bSigma}{\symbf{\Sigma}}
\newcommand{\N}{\mathbfscr{N}}

## Introduction

The aim of this small file, which might later become an appendix of the main article, is to explore a simple multivariate example in Stan language. I base my code on the example developed by @McElreath2020 (chap. 14) about a cafe robot.

Let a robot visit cafes and order a coffee. The robot measures each time how long did it wait, and when it arrived (a simple categorical variable of two values: morning and afternoon). In the chapter 13 of @McElreath2020, it has been shown that the robot learns better when using partial pooling. In the chapter 14, McElreath claims that we can learn even more by also investigating and accounting for the correlation structure. Why? Because popular cafes have a longer waiting time in the morning (rush hour) and improves significantly the afternoon, while there is little difference in waiting time between the morning and the afternoon for non-popular cafes. Therefore, there are two levels of borrowing information:

1. Between the cafes (partial pooling)
2. Between the parameters (covariance)

The @sec-create_data is dedicated to simulate data, which are fit in @#sec-fit by jointly modelling slopes and intercepts. The results are analysed in @sec-results.

## Create data {#sec-create_data}

```{r}
#| output: false
#### Clear space and load packages
rm(list = ls())
graphics.off()

options(max.print = 500)

library(data.table)
library(cmdstanr)
	register_knitr_engine(override = TRUE)
library(ellipse)
library(MASS)
library(gt)
```

The parameters are defined as below:
```{r}
#### Population parameters, p. 437
a = 3.5 # Mean intercept (i.e., mean waiting time)
b = -1 # Mean slope (i.e., mean improvement waiting time in the afternoon. Improvement because < 0)

sigma_a = 1 # Variance intercept
sigma_b = 0.5 # Variance change morning/afternoon (improvement if < 0, worsen otherwise)
sigma_cafe = 0.15 # Variance within cafe

rho = -0.7 # Covariance between intercepts and slopes

#### Var-Cov matrix
sigma_mat = diag(c(sigma_a, sigma_b))
rho_mat = matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)

varCov_mat = sigma_mat %*% rho_mat %*% sigma_mat
```

and the data are created using a multivariate normal distribution for the parameters (intercept and slope for each cafe, all coming from the same distribution---group effect and correlation are then included):

```{r}
#### Simulate data
## Simulate parameters
n_cafes = 40
set.seed(5)

params_dt = as.data.table(mvrnorm(n_cafes, c(a, b), varCov_mat))
setnames(params_dt, new = c("alpha_cafe", "beta_cafe"))

plot(params_dt[, alpha_cafe], params_dt[, beta_cafe], pch = 19, xlab = "alpha", ylab = "beta", axes = FALSE)
axis(1)
axis(2, las = 1)
for (l in c(0.1, 0.3, 0.5, 0.8, 0.99))
	lines(ellipse(x = varCov_mat, centre = c(a, b), level = l), col = "#22442233", lwd = 2)

## Simulate waiting times
set.seed(22)
n_visit = 10 # 5 in the morning, 5 in the afternoon

if (n_visit %% 2 != 0)
	stop("n_visit must be an even integer")

visit_dt = data.table(cafe = rep(1:n_cafes, each = n_visit),
	is_afternoon = rep(c(rep(FALSE, n_visit/2), rep(TRUE, n_visit/2)), n_cafes))
setkey(visit_dt, cafe)

for (i in 1:n_cafes)
	visit_dt[.(i), waiting_time := rnorm(n = n_visit,
		mean = params_dt[i, alpha_cafe] + params_dt[i, beta_cafe]*is_afternoon, sd = sigma_cafe)]

visit_dt[, afternoon_int := ifelse(is_afternoon, 1, 0)]
```

This creates an easy, balanced dataset of `{r} n_cafes` cafes visited `{r} n_visit` times (equaly shared between mornings and afternoons).

## Fit the model {#sec-fit}
### DAG
```{mermaid}
%%| label: fig-model
%%| fig-cap: Diagram Acyclic Graph
graph TB
	Y("$$Y \sim N \left( \mu_{\text{cafe}[i]}, \sigma_{\text{cafe}[i]} \right)$$")
```

### Stan model
Here is the stan model according to the @fig-model:
```{stan output.var = "model", cache = TRUE, fold = FALSE}
data {
	// Dimensions
	int <lower = 1> N_cafes; // Number of cafes
	int <lower = 1> N_visit; // Number of visits per cafe
	int <lower = N_cafes*N_visit, upper = N_cafes*N_visit> N; // Number of data

	// Explanatory variable
	vector <lower = 0, upper = 1> [N] afternoon; // 0 if morning, 1 otherwise

	// Observations
	vector <lower = 0> [N] wait;
}

parameters {
	// Population parameters...
	// ... intercept and slope
	real a; // Intercept
	real b; // Slope
	
	// ... variances
	real sigma_a;
	real sigma_b;
	cholesky_factor_corr[2] L;

	// Variance (residuals)
	real<lower = 0> sigma; // Variance within same cafe

	// Whatever
	vector [N_cafes] a_cafe;
	vector [N_cafes] b_cafe;
}

transformed parameters {
	vector <lower = 0>[2] sigma_vec = to_vector({sigma_a, sigma_b});
}

model {
	int cc = 0;

	// Population priors
	target += normal_lpdf(a | 5, 2);
	target += normal_lpdf(b | -1, 2);
	
	target +=  lkj_corr_cholesky_lpdf(L | 2); // It contains rho (non-diag), and 1 (diag)

	// Variance (residuals)
	target += exponential_lpdf(sigma | 1);

	// Likelihood and cafe parameters
	for (i in 1:N_cafes)
	{
		target += multi_normal_cholesky_lpdf(to_vector({a_cafe[i], b_cafe[i]}) | to_vector({a, b}),
			diag_pre_multiply(sigma_vec, L)); // Cafe params
		for (j in 1:N_visit)
		{
			cc = (i - 1)*N_visit + j;
			target += normal_lpdf(wait[cc] | a_cafe[i] + b_cafe[i]*afternoon[cc], sigma); // Likelihood
		}
	}
}

generated quantities { // This is to recover the variance-covariance matrix instead of having its Cholesky factor
	corr_matrix[2] Sigma;
	Sigma = multiply_lower_tri_self_transpose(diag_pre_multiply(sigma_vec, L));
}
```

All the functions are described in the [Stan Functions Reference](https://mc-stan.org/docs/functions-reference/), however, few explanations are required for the multivariate.

Remember that the variance-covariance matrix of a multivariate distribution, $\bSigma$, can be rewritten as a product of 'pure variance' and 'pure covariance' matrices:
$$
\bSigma = \begin{bmatrix} \sigma_a & 0 \\ 0 & \sigma_b \end{bmatrix}
	\begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}
	\begin{bmatrix} \sigma_a & 0 \\ 0 & \sigma_b \end{bmatrix}
$$

The diagonal elements of $\bSigma$ are the variances of the parameters ($\sigma_a^2$ and $\sigma_b^2$), and the non-diagonal elements are the covariances ($\rho \sigma_a \sigma_b$). We denote the diagonal matrix by $\symbf{V}$, and the middle matrix by $\bOmega = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}$. The matrix $\bOmega$ is symmetric positive-definite and can be Cholesky-decomposed:
$$
\bOmega = \symbf{L}\symbf{L}^T,
$$
where the index $T$ is for transpose, and $L$ is a lower triangular matrix. According to [Stan correlation matrix page](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html), it is much better computationally to work directly with the Cholesky factor of $\bSigma$, which is:
\begin{equation}
	\label{eq::cholesky_Sigma}
	\begin{aligned}
		\bSigma &= \symbf{V} \bOmega \symbf{V} \\
			&= \symbf{V} \symbf{L} \symbf{L}^T \symbf{V} \\
			&= (\symbf{V} \symbf{L}) (\symbf{V} \symbf{L})^T.
	\end{aligned}
\end{equation}
We denote by $\symbf{L_{\Sigma}} = \symbf{V} \symbf{L}$ the Cholesky factor of $\bSigma$.

Now that this is explained, the line `target +=  lkj_corr_cholesky_lpdf(L | 2);` is just the prior on $\symbf{L}$. The prior `target +=  lkj_corr_lpdf(\eta)` just put a prior that is skeptical about high correlation for $\eta \geqslant 1$. The higher $\eta$, the more concentrated around the identity matrix is the prior [@McElreath2020]. [Stan correlation matrix page](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html) recommands to work with the Cholesky decomposition, hence the prior on $\symbf{L}$ rather than $\bOmega$ and the use of `lkj_corr_cholesky_lpdf` instead of `lkj_corr_lpdf` (note the _cholesky_ that disappeared in the name).

Then the second step is `diag_pre_multiply(sigma_vec, L)`. This function just transforms a vector (here $(\sigma_a, \sigma_b)$) to a diagonal matrix and then multiply it by $\symbf{L}$, which gives $\symbf{L_{\Sigma}}$ (see equation \eqref{eq::cholesky_Sigma}). Note that this is valid only because I use `multi_normal_cholesky_lpdf` which requires the Cholesky factor of $\bSigma$, $\symbf{L_{\Sigma}}$, contrary to `multi_normal_lpdf`.

### Stan data
```{r}
#### Fit the model
## Stan data
stanData = list(
	N_cafes = n_cafes, # Number of cafes
	N_visit = n_visit, # Number of visits per cafe
	N = n_cafes*n_visit,
	afternoon = visit_dt[, afternoon_int],
	wait = visit_dt[, waiting_time]
)

## Common variables
n_chains = 4

iter_warmup = 1000
iter_sampling = 1000
```

###  the model
```{r}
#| output: false
#| fold: false
## Run
fit = model$sample(data = stanData, chains = n_chains, parallel_chains = ifelse(n_chains < 4, n_chains, 4),
	seed = NULL, refresh = 250, max_treedepth = 12, save_warmup = TRUE,
	iter_sampling = iter_sampling, iter_warmup = iter_warmup, adapt_delta = 0.8)
```

## Results {#sec-results}

```{r}
corr_mat = fit$draws("Sigma", inc_warmup = FALSE)
corr_mat = matrix(apply(X = corr_mat, MARGIN = 3, FUN = mean), ncol = 2)

test_mat = fit$draws("Test", inc_warmup = FALSE)
test_mat = matrix(apply(X = test_mat, MARGIN = 3, FUN = mean), ncol = 2)

a_cafes = apply(X = fit$draws("a_cafe", inc_warmup = FALSE), MARGIN = 3, FUN = mean)
b_cafes = apply(X = fit$draws("b_cafe", inc_warmup = FALSE), MARGIN = 3, FUN = mean)

params_dt[, a_est := a_cafes]
params_dt[, b_est := b_cafes]

L_mat = fit$draws("L", inc_warmup = FALSE)
L_mat = matrix(apply(X = L_mat, MARGIN = 3, FUN = mean), ncol = 2)

L_mat %*% t(L_mat)
```
